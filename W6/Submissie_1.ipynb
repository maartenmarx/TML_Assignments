{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f905aa96",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "privacy",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment Applied Machine Learning BSc IK \n",
    "\n",
    "## Notebook made by\n",
    "\n",
    "**Gebruik graag dit formaat**\n",
    "\n",
    "* Voor de namen:  voornaam rest van je naam, voornaam rest van je naam,....\n",
    "* je studentnummers: hetzelfde: scheidt met `,`\n",
    "* je emails: hetzelfde: scheidt met `,`\n",
    "* voor je groep: **alleen de hoofdletter** (iets als  `A` of `B` dus)\n",
    "\n",
    "__Namen__:Anoniem",
    "\n",
    "__Emails__:Anoniem",
    "\n",
    "__Student id__:Anoniem",
    "\n",
    "__Groep__:Anoniem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fb9c6",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* Een aantal opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. Dit zijn dus *NIET* alle tests, die komen er bij het graden nog bij.\n",
    "* Elke vraag is 1 punt waard, tenzij anders aangegeven. Soms is die punt onderverdeeld in deelpunten, maar niet altijd. \n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren\n",
    "\n",
    "## Na het inleveren!\n",
    "\n",
    "* Het gebeurt erg vaak dat mensen een \"leeg bestand\" inleveren. Vaak een andere versie van de opgave die nog ergens op je computer rondslingerde. Zonde van al je werk toch!\n",
    "* Dus, lever **minstens een half uur voor tijd in**. Download dan wat je hebt ingeleverd op Canvas. Geef het een andere naam om verwarring te voorkomen. En draai alle cellen, en bekijk het. Geen syntax fouten? Alle vragen gemaakt? Dan zit het vast wel goed, en hoef je niet in de zenuwen te zitten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9576e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd80cdc0e19a67f9bff05d0e08a99c5e",
     "grade": false,
     "grade_id": "i",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Applied Machine Learning W6\n",
    "\n",
    "In the exercises of this week we will be looking more closely at different Machine Learning models, such as decision trees and random forests. We will also be taking our first look at building models for the text domain, which will also be part of the assignment for next week.\n",
    "\n",
    "\n",
    "## Assignment Index\n",
    "\n",
    "- 1.1 [Decision Trees and Random Forests](#random_forests)\n",
    "- 1.2 [Logistic Regression with the Titanic dataset](#titanic)\n",
    "- 1.3 [Text Classification](#text_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12dd95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "705b25f855cc865df255378a28191950",
     "grade": false,
     "grade_id": "imp",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# testing\n",
    "from numpy.testing import *\n",
    "from nose.tools import assert_count_equal, assert_equal, assert_almost_equals\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "# Please do not remove this: \n",
    "np.random.seed(31415)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ac698",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9d02218d9292fa96a5ab35315634db7",
     "grade": false,
     "grade_id": "cell-175428cab90f9597",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"random_forest\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69573585",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c18bdac2685c720bdf5309d57cac4dd6",
     "grade": false,
     "grade_id": "dt",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Decision trees and random forests\n",
    "\n",
    "In this for exercise we will be working with decision trees on the familiar digits dataset. We will be using a random forest classifier to classify the images, and do a little exploration on the mistakes that the classifier makes.\n",
    "\n",
    "* Create a random forest classifier for the digits dataset, with a 80-20 test train split.\n",
    "* Evaluate with a report of P, R and F1 for each digit (class) and a confusion matrix.\n",
    "* Not only predict a class, but also compute the probabilities for each class using `predict_proba`.\n",
    "  - You can use `predict_proba` and make binary labels from it, so you only have to run the \n",
    "  prediction code once.\n",
    "* Make an insightful investigation of this probability space.\n",
    "    * which ones are more easily confused (eg, the probability of the winner is not that high and not that far off from the numbefr 2.)\n",
    "    * Write a small report, with insightful tables and graphics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e506cf6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "654c88a022ed1aae603f28291f672844",
     "grade": false,
     "grade_id": "cell-0c8deb7a9b103313",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We give this to you, we load in the dataset and we show a small example of the dataset.\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "# set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels to get a feel for what is in the digits dataset.\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(31415)# Do not remove this\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set up the data and split up in train and test\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 15):\n",
    "#     forest = RandomForestClassifier(max_depth=i, random_state=0).fit(X_train, y_train)\n",
    "#     print(f\" R2 score of test: {forest.score(X_test, y_test)}, with max_depth {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ee440",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, 15)\n",
    "y = [RandomForestClassifier(max_depth=i, random_state=0).fit(X_train, y_train).score(X_test, y_test) for i in x]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"R2 score\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74b560-a223-4f5e-b0f5-ec4b73cb1b25",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4568b5eeff83b9df1c2bfd3f88b88c4",
     "grade": false,
     "grade_id": "cell-c53c408eda935531",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Next is making the classifier and predicting the labels of the digits\n",
    "forest = RandomForestClassifier(max_depth=3, random_state=0).fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "y_prob = forest.predict_proba(X_test)\n",
    "\n",
    "predicted_probabilities = y_prob\n",
    "predicted_labels = y_pred\n",
    "digits_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "digits_confusion_matrix, predicted_probabilities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19849e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(data=pd.DataFrame(digits_confusion_matrix), annot=True)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb28c7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a7c45744350355feb7cefdd2140b7d8",
     "grade": true,
     "grade_id": "cell-b4efb574acdcabde",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(X_train.shape, (1437, 64))\n",
    "assert_equal(X_test.shape, (360, 64))\n",
    "assert_equal(y_train.shape, (1437,))\n",
    "assert_equal(y_test.shape, (360,))\n",
    "assert_equal(predicted_probabilities.shape, (360, 10))\n",
    "assert_equal(predicted_labels.shape, (360,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c1c86d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e19ca0ea2277dcd1450a13a0b2786b4",
     "grade": false,
     "grade_id": "cell-965112fc0d83ac9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below, do a small report on the predicted probabilities, showing easy and hard to classify instances. Also fill in the `most_confused` variable, which should be a list of ten elements, where for each digit you report the digit it was confused by most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_prob = pd.DataFrame(y_prob)\n",
    "df_prob['class'] = y_test\n",
    "\n",
    "arr =  np.full((10, 10), np.nan)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        arr[i][j] = df_prob[df_prob['class'] == i].mean()[j]\n",
    "np.fill_diagonal(arr, 0)\n",
    "\n",
    "df_confused = pd.DataFrame(arr)\n",
    "most_confused = [df_confused[i].idxmax() for i in df_confused.index] # Fill this in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ff16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=df_confused)\n",
    "plt.xlabel('Most confused')\n",
    "plt.ylabel('True numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658505b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "np.fill_diagonal(matrix, 0)\n",
    "confused = np.argsort(matrix, axis=1)\n",
    "\n",
    "most_confused = confused[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f56e78",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9837aa26d0502d1e6724f0066d7032d1",
     "grade": false,
     "grade_id": "cell-30cd51f7f488eb7d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Also make a nice report on the scores\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Show this in a nice dataframe\n",
    "pd.DataFrame({'Class': range(10), 'Most Confused With': most_confused})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3bb590",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a520eee1f743edf7ca0e4fd91d90269",
     "grade": true,
     "grade_id": "cell-69d04e2935ea567e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54472555",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40544647a1acaafe50370e89f9550ea1",
     "grade": false,
     "grade_id": "cell-20aef2210d19741c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a small report about the predictions of the classifier using `predict_proba`, and discuss the probability space of each true digit class. For instance, argue, using the probabilities which digit class is the easiest to predict correctly, and which one is the hardest. What properties of the probability space make you think it is easy or hard? Don't write a lot, make good pictures. Make the code in the empty cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da7209",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "066e71b7c06a3a9f673aeccc79e17699",
     "grade": true,
     "grade_id": "cell-2fb61c1e1933993d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "differences = [(max(predicted_probabilities[i]) - sorted(predicted_probabilities[i], reverse=True)[1]) for i in range(10)]\n",
    "digit_easy = [i for i in range(len(differences)) if differences[i]==max(differences)][0]\n",
    "digit_hard = [i for i in range(len(differences)) if differences[i]==min(differences)][0]\n",
    "print(\"The easiest digit to predict correctly was {0}\".format(digit_easy))\n",
    "print(\"The hardest digit to predict correctly was {0}\".format(digit_hard))\n",
    "\n",
    "df = pd.DataFrame({'differences': differences})\n",
    "bar = sns.barplot(data=df, x=df.index, y=df['differences'])\n",
    "\n",
    "# As you can see, the peaks in the barplot represent the digits where there was next to no uncertainty of what digit it was\n",
    "# However, there were a few digits, like 5, 7 and 9 where there was a lot of ambiguity if it was that particular digit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03820f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d2d480038fc8fed3d6ee5a17ffd4c73",
     "grade": false,
     "grade_id": "cell-260ed38a9d7b2f35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"titanic\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9958ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21387af09a5062d9803b79d572a8cc84",
     "grade": false,
     "grade_id": "tita",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Logistic Regression for the Titanic Dataset\n",
    "\n",
    "In exercise we are going to use logistic regression on the Titanic dataset, similar to what we did last week with linear regression. We are mostly going to perform the same steps as last week, performing a grid search on parameters, and comparing the performance of different models. We are also going to do a bit more in-depth preprocessing, working with categorical variables.\n",
    "\n",
    "## P1 standardization & Categorical Variables\n",
    "\n",
    "* Load the titanic dataset.\n",
    "* Impute the missing data.\n",
    "* Split the dataset into a train and test set with an 80/20 split.\n",
    "* Z-transform all explanatory variables.\n",
    "* Now use logistic  regression to predict survived from the other variables.\n",
    "* Output the regression weights, and compute Rsquare and RMSE.\n",
    "\n",
    "## P2 Grid Search\n",
    "\n",
    "* Split titanic into train, validation, test.\n",
    "* Again, scale the data.\n",
    "* Use the validation set to find optimal values for the the relevant parameters.\n",
    "* Tabulate or plot the outcomes in a handy and insightful manner, \"displaying the grid of possibilities\".\n",
    "* Pick the best model from the validation set, and run this on the the test set.\n",
    "\n",
    "## P3 More Models\n",
    "\n",
    "* Train two other models, SVM and  random forest, as described in the Data Science Handbook notebooks.\n",
    "* Report the scores of the three models in a meaningful way, and conclude.\n",
    "* Are you able to make confidence intervals? Can you say something about significant differences?\n",
    "* Can you indicate where improvements are made, if any?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60e8a0-1c7a-4a35-b395-a87347994e9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3562c66ba70ed04827135dbd374b53bf",
     "grade": false,
     "grade_id": "cell-0ae7902a5b183292",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec504d-4518-41c4-a171-6bea975d5dd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d37614681fdd26d317430d9a784b944a",
     "grade": false,
     "grade_id": "cell-cc402f39b6b44416",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We will load the dataset for you\n",
    "titanic = sns.load_dataset('titanic')\n",
    "# Drop the alive column, this is just another version of the survived column.\n",
    "titanic.drop(['alive', 'pclass', 'embarked', 'who', 'alone', 'adult_male', 'sibsp', 'sex'], axis=1, inplace=True)\n",
    "# For this exercise we are going to work on a subset of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb48c8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3675dbef84dad25016513a92c41f303",
     "grade": false,
     "grade_id": "cell-6cca4dc4310de8d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae4224",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf10be9dbfdeec372cbd72a8ec9e09d7",
     "grade": false,
     "grade_id": "cell-a68c12272b0495dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## P1 standardization & Categorical Variables\n",
    "\n",
    "Here we will prepare the data for classification and run a first classifier, we will perform several steps, including the one-hot encoding of the categorical variables.\n",
    "\n",
    "* Load the titanic dataset.\n",
    "* Impute the missing data, use the median for the age column, and the mode for the other two column, think about why we do this.\n",
    "* Split the dataset into a train and test set, using an 80/20 split.\n",
    "* Z-transform all explanatory variables.\n",
    "* Now use logistic  regression to predict survived from the other variables.\n",
    "* Output the regression weights, and compute Rsquare and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of nan values in age: {titanic['age'].isna().sum()}\")\n",
    "print(f\"Amount of nan values in deck: {titanic['deck'].isna().sum()}\")\n",
    "print(f\"Amount of nan values in embark_town: {titanic['embark_town'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['age'].mean(), titanic['deck'].mode()[0], titanic['embark_town'].mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5fa71d",
   "metadata": {},
   "source": [
    "> We can't take the mean of a string. That's why we use the mode ```ValueError: could not convert string to float:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b95497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "np.random.seed(31415)# Do not remove this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c08dd-c632-4668-bea8-952990288a5e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43c7b1a9230fdcece0fe28cc68d0184e",
     "grade": false,
     "grade_id": "cell-ce8c7f217556cf2c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Do the imputation of the relevant columns below\n",
    "titanic['age'] = titanic['age'].fillna(round(titanic['age'].mean(), 0)) # impute with median\n",
    "titanic['deck'] = titanic['deck'].fillna(titanic['deck'].mode()[0]) # impute with mode\n",
    "titanic['embark_town'] = titanic['embark_town'].fillna(titanic['embark_town'].mode()[0]) # impute with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "imp_mode = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "imp_mean.fit(titanic[['age']])\n",
    "titanic['age'] = imp_mean.transform(titanic[['age']])\n",
    "\n",
    "imp_mode.fit(titanic[['deck', 'embark_town']])\n",
    "titanic[['deck', 'embark_town']] = imp_mode.transform(titanic[['deck', 'embark_town']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbb500",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of nan values in age: {titanic['age'].isna().sum()}\")\n",
    "print(f\"Amount of nan values in deck: {titanic['deck'].isna().sum()}\")\n",
    "print(f\"Amount of nan values in embark_town: {titanic['embark_town'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1afeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna is faster than SimpleImputer\n",
    "print(f\"CPU times fillna: total: {15.6} ms\")\n",
    "print(f\"CPU times Imputer: total: {31.2} ms\")\n",
    "print(f\"Wall time fillna: {8.05} ms\")\n",
    "print(f\"Wall time Imputer: {28} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c630d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0dd5b36dd453565639f7fb033742643d",
     "grade": false,
     "grade_id": "cell-79983b9eb5ac73b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Categorcial Features\n",
    "\n",
    "The titanic dataset contains several categorical features, and we want to use them for our logistic regression. We already touched on this earlier, and mentioned that you can do this using one hot encodings. Fortunately, scikit learn offers easy functions to do this. We will be using a nice sklearn function to do our preprocessing, but also take a look at the code in the cell below and in particular the shapes, and see if you understand why our categorical features would have that shape now.\n",
    "\n",
    "Please fill in the code below where you create one hot encodings for the categorical features, and fill in the `categorical_features` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed272bb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9480ef83d9e22863d1b336333e388c9c",
     "grade": false,
     "grade_id": "cell-93d55293e5fb25d7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_features = ['class', 'deck', 'embark_town'] # Fill in the colum names of the categorical features\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "categorical_feature_matrix = []\n",
    "for feature in categorical_features:\n",
    "    one_hot_encoded_feature = encoder.fit_transform(titanic[feature].to_numpy().reshape(-1, 1))\n",
    "    # Understand the new shape of the feature?\n",
    "    print(one_hot_encoded_feature.shape)\n",
    "    # now add it to the dataset and drop the original feature\n",
    "    categorical_feature_matrix.append(one_hot_encoded_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8d79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in categorical_features:\n",
    "    print(f\"{feature}: {titanic[feature].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2736440f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccae78abc1c88224c776c52b262c217e",
     "grade": true,
     "grade_id": "cell-399a24e4d271aa76",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aedd436c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baacdba8d7c923e541ad72cc8f253c6f",
     "grade": false,
     "grade_id": "cell-8a03e06eca1d9adf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next up we will divide the data up into train and test, and scale the data using the familiar `StandardScaler`. Remember, we don't want to do this on the categorical features. The easiest way to do this is to get the indices of train and test from `train_test_split`, so that you can select the right indices from both the continous and categorical arrays. Of course you don't have to do anything with the `survived` column, as this will be our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1cdd3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a57b70b7cf250188a65782aef7eb1892",
     "grade": false,
     "grade_id": "cell-7d161d16981dff1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# look at this function, it works like a Pipeline object, and is super helpful in \n",
    "# our case, where we want to to apply different transformations to different columns\n",
    "# There is also a really nice example in the documentation! We also want to use the StandardScalar and OneHotEncoder\n",
    "# (although a bit unclear, you can give a list with multiple features for each of the transformers)\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "continuous_features = ['age', 'parch', 'fare'] # fill in the categorical features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic[continuous_features + categorical_features],\n",
    "                                                    titanic['survived'], test_size=0.20, stratify=titanic['survived'])\n",
    "\n",
    "# fill this in, using the right transformer for the right columns with the lists you already made\n",
    "scaler_and_one_encoder = make_column_transformer(\n",
    "    (StandardScaler(), continuous_features),\n",
    "    (OneHotEncoder(), categorical_features)\n",
    ") \n",
    "\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "X_train_scaled_and_encoded = scaler_and_one_encoder.fit_transform(X_train)\n",
    "# With fit_transform you will get a bad shape\n",
    "X_test_scaled_and_encoded = scaler_and_one_encoder.transform(X_test)\n",
    "#Now we Z transform the variables\n",
    "X_train_scaled_and_encoded.shape, X_test_scaled_and_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d5777",
   "metadata": {},
   "source": [
    "> Always fit transform the train data, but never fit the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_and_one_encoder = make_column_transformer(\n",
    "    (StandardScaler(), continuous_features),\n",
    "    (OneHotEncoder(sparse_output=False), categorical_features)\n",
    ") \n",
    "\n",
    "X_train_scaled_and_encoded = scaler_and_one_encoder.fit_transform(X_train)\n",
    "# With fit_transform you will get a bad shape\n",
    "X_test_scaled_and_encoded = scaler_and_one_encoder.transform(X_test)\n",
    "#Now we Z transform the variables\n",
    "X_train_scaled_and_encoded.shape, X_test_scaled_and_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de13381e",
   "metadata": {},
   "source": [
    "> Shape is 16 because: we start with 6 columns. 3 will be removed when encoded. 13 will be added because of the encoder.\n",
    "$(6 - 3) + 13 = 16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda75cf-773c-4f8d-b289-021ebdb2e124",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62f56dc1545232515f96453c795ea0ae",
     "grade": true,
     "grade_id": "cell-e7008c5b555cea7b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(titanic['age'].isna().sum(), 0)\n",
    "# are the sizes of the train and test sets correct?\n",
    "assert_equal(X_train_scaled_and_encoded.shape, (712, 16))\n",
    "assert_equal(X_test_scaled_and_encoded.shape, (179, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506524b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07ffedbb535be2d4ed89cac9cc902b2b",
     "grade": false,
     "grade_id": "cell-48dde941fcafc19f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have processed the data, we will run the logistic classification model, complete the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4073bfc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77cd80eeb95ae2a701c89e58e9ed5f5a",
     "grade": false,
     "grade_id": "cell-c422b8822caeed66",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression().fit(X_train_scaled_and_encoded, y_train)\n",
    "predictions = logistic_model.predict(X_test_scaled_and_encoded)\n",
    "regression_weights = logistic_model.coef_\n",
    "r2 = r2_score(y_test, predictions)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=True)\n",
    "\n",
    "r2, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357172fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e68b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ab2477213dae33cc66ed9e2aa2a8907",
     "grade": true,
     "grade_id": "cell-187a6c9889346d8e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(predictions.shape[0], 179)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d45b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bea99d82266a1e6c860012910712f5cf",
     "grade": false,
     "grade_id": "cell-5a2d4cfa4eeed89b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## P2 Grid Search\n",
    "\n",
    "Here, do a grid search over the parameters of the Logistic Regression model, we have already given the ranges\n",
    "for the variables that we want you to try, for this, split the data into train, and test, and use this training with the gridsearch function with the cross validation set to 5. Use the data you created in the previous exercise.\n",
    "\n",
    "HINT: sklearn does not have a function to directly split a dataset into train, validation and test, but you can use `train_test_split` twice, once to create separate train and test sets, and then once more to split the train set into a train and validation portion. Do a 60/20/20 split.\n",
    "\n",
    "Fill in the `best_params` variable, which should be a dict that contains for each parameter the best option that you found. You might get some warnings about convergence, but you can leave these as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "X_val_cross, X_test_cross, y_val_cross, y_test_cross = train_test_split(X_test_scaled_and_encoded,\n",
    "                                                                    y_test, test_size=0.50, stratify=y_test)\n",
    "# Define parameter grid for grid search\n",
    "param_grid = {'penalty': ['l1', 'l2'],\n",
    "              'C': [0.00001, 0.01, 0.1, 1, 10],\n",
    "              'solver': ['liblinear', 'saga']}\n",
    "\n",
    "np.random.seed(31415) # Do not remove this\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "\n",
    "best_params = dict()\n",
    "best_score = 0\n",
    "CV = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture --no-stdout\n",
    "# Perform grid search with cross-validation on the validation set\n",
    "# to get the best model\n",
    "\n",
    "for penalty in param_grid['penalty']:\n",
    "    for C in param_grid['C']:\n",
    "        for solver in param_grid['solver']:\n",
    "            logreg = LogisticRegression(random_state=42, penalty=penalty, C=C, solver=solver)\n",
    "            scores = cross_val_score(logreg, X_val_cross, y_val_cross, cv=CV)\n",
    "            \n",
    "            score = np.mean(scores)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'penalty' : penalty,\n",
    "                               'C' : C,\n",
    "                               'solver': solver}  \n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ede3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture --no-stdout\n",
    "\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=CV)\n",
    "grid_search.fit(X_val_cross, y_val_cross)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106e5ec-e44d-42f7-b076-d2e18006821d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcd6a03e075149c169be977d1bc1eb14",
     "grade": false,
     "grade_id": "cell-c108ccc33931ba14",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(31415)# Do not remove this\n",
    "\n",
    "# Run the best model on the test set\n",
    "logreg = LogisticRegression(**best_params, random_state=42)\n",
    "best_model_predictions = logreg.fit(X_train_scaled_and_encoded, y_train).predict(X_test_scaled_and_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, best_model_predictions)\n",
    "rmse = mean_squared_error(y_test, best_model_predictions)\n",
    "\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f868e0-613f-4a89-8b8d-6db38391b1bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1e6ccc400493a444e13d0a9cb6c7f8a",
     "grade": true,
     "grade_id": "cell-a08bbae2502e17c1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(best_params), dict)\n",
    "assert_equal(best_model_predictions.shape[0], 179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c493bb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bd0a8387ef9a71a37e0c20a70359a50",
     "grade": true,
     "grade_id": "cell-2aff6aeef568b100",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c0e70b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c8423c3104bb0eb4c0a7b4aa8340be2",
     "grade": false,
     "grade_id": "cell-29a8e08f3945a347",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## P3 More Models\n",
    "\n",
    "* Train two other models, SVM and  random forest, as described in the Data Science Handbook notebooks.\n",
    "* Report the scores of the three models using `weighted average` for P, R and F1, and conclude.\n",
    "* Can you indicate where improvements are made, if any?\n",
    "* Give a report with a dataframe containing P, R and F1 scores per class for each model, and a dataframe with the confidence intervals of the models. See on example below of how it should look like. You can use the train test splits that you created in part 1 for this exercise.\n",
    "\n",
    "|                     |   precision |   recall |   f1-score |\n",
    "|:--------------------|------------:|---------:|-----------:|\n",
    "| Logistic Regression |        -    |     -    |       -    |\n",
    "| SVM                 |        -    |     -    |       -    |\n",
    "| Random Forest       |        -    |     -    |       -    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44de2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Evaluate models on the test set\n",
    "svm_predictions = SVC().fit(X_train_scaled_and_encoded, y_train).predict(X_test_scaled_and_encoded)\n",
    "random_forest_predictions = RandomForestClassifier().fit(X_train_scaled_and_encoded, y_train).predict(X_test_scaled_and_encoded)\n",
    "logreg_predictions = LogisticRegression().fit(X_train_scaled_and_encoded, y_train).predict(X_test_scaled_and_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ceecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Logistic Regression', 'SVM', 'Random Forest']\n",
    "scores=['precision', 'recall', 'f1-score']\n",
    "\n",
    "df = pd.DataFrame(index=models, columns=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73331e5d-9319-4cfe-ad40-be62bc806b17",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "342f1e35dc6e5bf6989584e324c49e1d",
     "grade": false,
     "grade_id": "cell-46fb1bca58652f1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df.loc['Logistic Regression', 'precision'] = precision_score(y_test, logreg_predictions, average='weighted')\n",
    "df.loc['Logistic Regression', 'recall'] = recall_score(y_test, logreg_predictions, average='weighted')\n",
    "df.loc['Logistic Regression', 'f1-score'] = f1_score(y_test, logreg_predictions, average='weighted')\n",
    "\n",
    "df.loc['SVM', 'precision'] = precision_score(y_test, svm_predictions, average='weighted')\n",
    "df.loc['SVM', 'recall'] = recall_score(y_test, svm_predictions, average='weighted')\n",
    "df.loc['SVM', 'f1-score'] = f1_score(y_test, svm_predictions, average='weighted')\n",
    "\n",
    "df.loc['Random Forest', 'precision'] = precision_score(y_test, random_forest_predictions, average='weighted')\n",
    "df.loc['Random Forest', 'recall'] = recall_score(y_test, random_forest_predictions, average='weighted')\n",
    "df.loc['Random Forest', 'f1-score'] = f1_score(y_test, random_forest_predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e8dbd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5678231a5fd0b3f7cd477aa7f1b5d565",
     "grade": true,
     "grade_id": "cell-daa08f668859d9cc",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(svm_predictions.shape[0], 179)\n",
    "assert_equal(random_forest_predictions.shape[0], 179)\n",
    "assert_equal(logreg_predictions.shape[0], 179)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5dcaa",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cb1525197929d5b9d0147b37d6f9656",
     "grade": true,
     "grade_id": "cell-dc9c6fd961c89fca",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3d9d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4fd7a05bec4c0384582c4c72addb081",
     "grade": false,
     "grade_id": "cell-1b8a35db06db8cff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"text_classification\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d67258",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "debe1717d042fa9ec84bb35d4626d99b",
     "grade": false,
     "grade_id": "tc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Text classification\n",
    "\n",
    "We will now look at a famous set of texts, 20 newsgroups, which even comes with scikit learn. It contains a collection of documents/messages from 20 different Newsgroups, which are basically forums where people are discussing certain topics or asking questions. Each of the documents is labelled with the newsgroup it belongs to and the task is to determine the label of the document based on the contents of the document.\n",
    "\n",
    "1. Take a look at the code in [this blogpost](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a).\n",
    "    * The post does a number of things, and tries out two classifiers, Naive Bayes en SVM, only reports accuracy, and does a tiny bit of preprocessing. \n",
    "    * Surely you can do better! ;-)\n",
    "    \n",
    "We will expand on the notebook by making several changes/updates, all of which are described below, which we will do one by one. We have given you the original code from the notebook as a starting point, and we will add to it in the questions.\n",
    "\n",
    "Because of the size of the dataset, we will use a subsample of the data to make it a bit easier for you to work with.\n",
    "\n",
    "### 1 More elaborate evaluation\n",
    "\n",
    "You want to see the P, R and F1 for every class, and also the confusion matrix. Make it, and explain the findings. Which newsgroups are easy, which ones tend to get confused? Do not write down in words what is immediately visible in the plots and tables, but guide the reader and give insight.\n",
    "\n",
    "### 2 More learning algorithms\n",
    "\n",
    "Our all time favorite, logisctic regression of course, plus another of your choice. Compare the four you have now, and discuss. Do not just compare on accuracy. \n",
    "\n",
    "Do not just run a default and that's it. Optimize the relevant hyperparameters using a grid search.\n",
    "\n",
    "### 3 Feature engineering\n",
    "\n",
    "\n",
    "Do `CountVectorizer?`. OMG what a lot of possibilities! Make a much simpler, language independent representation by counting not words, but *character ngrams*. Do 2,3, and 4 grams. You may also play with the `max_df` and `min_df`. Play around a bit, and notice the difference in the number of dimensions.\n",
    "\n",
    "### 4 Baseline\n",
    "\n",
    "What should you consider an reasonable baseline for multiclass text classification? What algorithm and what representation of the text? Motivate your answer.\n",
    "\n",
    "### 5 Reduce dimensions\n",
    "\n",
    "Use PCA to bring all dimensions down to two, plot the documents in this new space and color them by their class, and report. Do you see the easy and hard to separate classes back here?\n",
    "\n",
    "### Extra \n",
    "If you are interested, use the parsimonious language models programmed in `https://github.com/larsmans/weighwords` to create a small language model for each class. Print the top 10-20 terms for each class. Do they make sense? Can you use these highly separating terms only for a classifier? Think of a simple implementation, and try it out.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3bcff-0ac2-4e5b-a237-24310f0e6012",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8100fd76ed298319fbe0fffb5f82a1ed",
     "grade": false,
     "grade_id": "cell-fc22d42e4b507204",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f47cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e70e64d5eb43d43b96aa294bc5a3b38",
     "grade": false,
     "grade_id": "cell-a1a94a65284424ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This will download the data, this can take a while.\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "# Let's take a look at the labels of the dataset\n",
    "print(twenty_train.target_names)\n",
    "print()\n",
    "#print the first document in the dataset\n",
    "print(twenty_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ebeef2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c335cba782d4411f18eafc7e9444b99",
     "grade": false,
     "grade_id": "cell-c77a5a7fefbc466d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below we have given you the original code from the blogpost where the SVM and SGD classifiers are implemented. Make sure you carefully read the blogpost, and that you really know what is going on here, this will help in the rest of the exercises. Note that with the parameters the way they are, the `SGDClassifier` acts as an SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8ef7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ed7f62eff452cdb9dd1acc1232b3750",
     "grade": false,
     "grade_id": "cell-80ed62d269d3d90f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf_mnb = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=0.05)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=0.05)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter_no_change=5, random_state=42)),\n",
    "])\n",
    "\n",
    "# Get the bayes predictions\n",
    "text_clf_mnb.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_mnb = text_clf_mnb.predict(twenty_test.data)\n",
    "\n",
    "# Get the SVM predictions\n",
    "text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "\n",
    "print(predicted_mnb.shape, predicted_svm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc554f72",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e79a69265f1d237fe236c4529028415",
     "grade": false,
     "grade_id": "cell-1d422a06ab545d4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1 More elaborate evaluation\n",
    "\n",
    "You want to see the P, R and F1 for every class, and also the confusion matrix. Make it, and explain the findings. Which newsgroups are easy, which ones tend to get confused? Do not write down in words what is immediately visible in the plots and tables, but guide the reader and give insight. Does it make sense that some of these categories might get confused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(twenty_test.target, predicted_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(twenty_test.target, predicted_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bd8a3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f9f96c55df846fc5a35fececb32be44",
     "grade": true,
     "grade_id": "cell-03f9026a7ee86d03",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "mnb_confusion_matrix = confusion_matrix(twenty_test.target, predicted_mnb)\n",
    "svm_confusion_matrix = confusion_matrix(twenty_test.target, predicted_svm)\n",
    "\n",
    "#plot MultinomialNB\n",
    "mnb_matrix = ConfusionMatrixDisplay(confusion_matrix=mnb_confusion_matrix,\n",
    "                              display_labels=text_clf_mnb.classes_)\n",
    "mnb_matrix.colorbar = False\n",
    "# Plot SVM\n",
    "svm_matrix = ConfusionMatrixDisplay(confusion_matrix=svm_confusion_matrix,\n",
    "                              display_labels=text_clf_svm.classes_)\n",
    "\n",
    "# Create a larger figure and axis\n",
    "fig, axes = plt.subplots(figsize=(16, 16), nrows=1, ncols=2)  # You can adjust the numbers to your desired size\n",
    "\n",
    "# Plot the confusion matrix on the larger axis\n",
    "mnb_matrix.plot(ax=axes[0], colorbar=False)\n",
    "svm_matrix.plot(ax=axes[1], colorbar=False)\n",
    "\n",
    "axes[0].set_title(\"Confusion Matrix for Multinomial Bayes Model\")\n",
    "axes[1].set_title(\"Confusion Matrix for Support Vector Machine Model\")\n",
    "\n",
    "# Set some nicer colorbars than the default\n",
    "cax_1 = fig.add_axes([axes[0].get_position().x1+0.01,axes[0].get_position().y0,0.02,axes[0].get_position().height])\n",
    "plt.colorbar(mnb_matrix.im_,  cax=cax_1)\n",
    "\n",
    "cax_2 = fig.add_axes([axes[1].get_position().x1+0.01,axes[1].get_position().y0,0.02,axes[1].get_position().height])\n",
    "plt.colorbar(svm_matrix.im_,  cax=cax_2)\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3fccc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b54c8ad736668b0fc5edf0129c0a735",
     "grade": false,
     "grade_id": "cell-c0467c59ab970ad3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write the text of your small report in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f9a7c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "685a4559593e4dc990f573fb7a228b4e",
     "grade": true,
     "grade_id": "cell-dd9686be04a9b3a5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The text of newsgroups 12 and 19 get confused the most and have a very low precision/accuracy/f1 rating while using SVM. \n",
    "\n",
    "When using MNB the precision of all newsgroups jumps up but this is not so much the case for recall and f1. \n",
    "\n",
    "It does make sense that a lot of these categories get confused, because the text of different newsgroups might be very similar to other newsgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee607f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d85c8c6e45d1d70c22642822599d99f",
     "grade": false,
     "grade_id": "cell-9fa65b26ac9acb8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2 More learning algorithms\n",
    "\n",
    "Our all time favorite, logisctic regression of course, plus another of your choice. Compare the four you have now, and discuss. Do not just compare on accuracy, report using for example P, R and F1. Also see how the author of the blogpost used a the `Pipeline` method. This is very elegant, so feel free to use it to implement your models here as well.\n",
    "\n",
    "Do not just run a default and that's it. Optimize the relevant hyperparameters using a grid search and report the scores of the best versions of both models that you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326cc0b8-bd45-41d7-9396-332e3731dd9a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c5f1f5791bdc4f2e1caa98a870545af",
     "grade": true,
     "grade_id": "cell-ed5cf0a99fb02dee",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_clf_logreg = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=0.05)),\n",
    "                            ('tfidf', TfidfTransformer()),\n",
    "                            ('clf', LogisticRegression(solver='sag')),\n",
    "                           ])\n",
    "\n",
    "text_clf_forest = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=0.05)),\n",
    "                            ('tfidf', TfidfTransformer()),\n",
    "                            ('clf', RandomForestClassifier(max_depth=5)),\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_clf_logreg.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_logreg = text_clf_logreg.predict(twenty_test.data)\n",
    "\n",
    "text_clf_forest.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_forest = text_clf_forest.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec78b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_logreg = {'penalty': ['l1', 'l2'],\n",
    "                     'C': [0.00001, 0.01, 0.1, 1, 10],\n",
    "                     'solver': ['liblinear', 'saga']}\n",
    "\n",
    "parameters_forest = {'max_depth': [1, 2, 3, 4],\n",
    "                     'min_samples_split': [1, 2, 3, 4],\n",
    "                     'max_features': [\"sqrt\", \"log2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clf_forest = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=0.05)),\n",
    "#                             ('tfidf', TfidfTransformer()),\n",
    "#                             ('clf', GridSearchCV(RandomForestClassifier(), parameters_forest)),\n",
    "#                             ])\n",
    "# text_clf_forest.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_mnb = precision_score(twenty_test.target, predicted_mnb, average='weighted')\n",
    "R_mnb = recall_score(twenty_test.target, predicted_mnb, average='weighted')\n",
    "F1_mnb = f1_score(twenty_test.target, predicted_mnb, average='weighted')\n",
    "\n",
    "P_svm = precision_score(twenty_test.target, predicted_svm, average='weighted')\n",
    "R_svm = recall_score(twenty_test.target, predicted_svm, average='weighted')\n",
    "F1_svm = f1_score(twenty_test.target, predicted_svm, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc0912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=['MNB', 'SVM'], columns=scores)\n",
    "df.loc['MNB', 'precision'] = P_mnb\n",
    "df.loc['MNB', 'recall'] = R_mnb\n",
    "df.loc['MNB', 'f1-score'] = F1_mnb\n",
    "\n",
    "df.loc['SVM', 'precision'] = P_svm\n",
    "df.loc['SVM', 'recall'] = R_svm\n",
    "df.loc['SVM', 'f1-score'] = F1_svm\n",
    "\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8beea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_logreg = text_clf_logreg.predict(twenty_test.data)\n",
    "predicted_model_four = text_clf_forest.predict(twenty_test.data)\n",
    "\n",
    "P_logreg = precision_score(twenty_test.target, predicted_logreg, average='weighted')\n",
    "R_logreg = recall_score(twenty_test.target, predicted_logreg, average='weighted')\n",
    "F1_logreg = f1_score(twenty_test.target, predicted_logreg, average='weighted')\n",
    "\n",
    "P_forest = precision_score(twenty_test.target, predicted_model_four, average='weighted')\n",
    "R_forest = recall_score(twenty_test.target, predicted_model_four, average='weighted')\n",
    "F1_forest = f1_score(twenty_test.target, predicted_model_four, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['Logistic Regression', 'precision'] = P_logreg\n",
    "df.loc['Logistic Regression', 'recall'] = R_logreg\n",
    "df.loc['Logistic Regression', 'f1-score'] = F1_logreg\n",
    "\n",
    "df.loc['Random Forest Classifier', 'precision'] = P_forest\n",
    "df.loc['Random Forest Classifier', 'recall'] = R_forest\n",
    "df.loc['Random Forest Classifier', 'f1-score'] = F1_forest\n",
    "\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf7527",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d19e686cd4585763ca0eeef0bebfb909",
     "grade": false,
     "grade_id": "cell-618a889982d9ad0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3 Feature engineering\n",
    "\n",
    "Do `CountVectorizer?`.There are  a lot of possibilities! Make a much simpler, language independent representation by counting not words, but *character ngrams*. Do 2,3, and 4 grams. You may also play with the `max_df` and `min_df`. Play around a bit, and notice the difference in the number of dimensions. \n",
    "\n",
    "Try with 2,3 and 4 grams,(so for each of these ONLY consider that ngram size and not the smaller ngrams) and fill in the code below, where you report the vocabulary size in the resulting training data for each ngram size, and put this value in `vocab_sizes`. Also, use `char` to get the character ngrams, and not `char_wb`!\n",
    "\n",
    "Think about what is happening, does the number of features increase or decrease, and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4] \n",
    "min_df_range = np.arange(0, 0.1, 0.01)\n",
    "max_df_range = np.arange(0.75, 1, 0.05)\n",
    "\n",
    "def count_vectorizer(max_n, min_df=0.05, max_df=0.95):\n",
    "    min_df = round(min_df, 2)\n",
    "    max_df = round(max_df, 2)\n",
    "    cv = CountVectorizer(ngram_range=(1, max_n), min_df=min_df, max_df=max_df)\n",
    "    cv.fit_transform(twenty_train.data)\n",
    "    print(f\"ngram_range({1},{max_n}): {cv.get_feature_names_out().shape}, df min({min_df}), max({max_df})\")\n",
    "    return {max_n: cv.get_feature_names_out().shape}\n",
    "    \n",
    "# [count_vectorizer(max_n, min_df, max_df) for max_n in x for min_df in min_df_range for max_df in max_df_range]\n",
    "vocab_sizes = [count_vectorizer(max_n) for max_n in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670085d-5328-41b0-895d-a5875821f803",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "648a9ab5d34c144941447f023742a846",
     "grade": false,
     "grade_id": "cell-864558d7883eb6ab",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_sizes = {2: 524, 3: 528, 4: 528}\n",
    "print(vocab_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db074f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6764da3f13b568eaa14f67952bae0f5a",
     "grade": true,
     "grade_id": "cell-da5fe80c60600410",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e568873",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9f870b511087bef889a3d164b4a0b33",
     "grade": false,
     "grade_id": "cell-1875194f379a69c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194dd27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30c9f3741f5f7d7b9b4fc8ce3d6747cb",
     "grade": false,
     "grade_id": "cell-44cd366c906ece98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the markdown cell below briefly discuss what an appropriate baseline for a text classification such as the one we are tackling in the NewsGroups20 dataset would be, and how you would represent text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dc801",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90d521aecae80c76aca95532ed80cd71",
     "grade": true,
     "grade_id": "cell-9ea55ad697b28e26",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "> We will get the mode of the text classification. Based on the matrix most of the classification will be 6, and we will use this as a baseline\n",
    "\n",
    "> An appropriate baseline would be a Bag of Words model to represent the text, this only returns the amount of times a word or words appear in text and ignores the order of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65280ad3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29ba6992542aa390e520cf48b99ceffe",
     "grade": false,
     "grade_id": "cell-b9ba9356d157c9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5 Reduce dimensions\n",
    "\n",
    "Use PCA to bring all dimensions down to two, plot the documents in this new space and color them by their class, and report. Do you see the easy and hard to separate classes back here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572debce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65d4d5680f87540f1f94ee54f7f5c642",
     "grade": true,
     "grade_id": "cell-a1c289aa9132123d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_transformer = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=0.05)),\n",
    "                     ('tfidf', TfidfTransformer())])\n",
    "                            \n",
    "# Transform text data with the pipeline\n",
    "X_train = text_transformer.fit_transform(twenty_train.data)\n",
    "X_test = text_transformer.transform(twenty_test.data)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train.toarray())\n",
    "X_test_pca = pca.transform(X_test.toarray())\n",
    "                        \n",
    "\n",
    "# You should end up with each sample having just two features.\n",
    "assert_equal(X_train_pca.shape[1], 2)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=twenty_train.target, cmap='viridis', s=20)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA 2D Projection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train.toarray())\n",
    "X_test_pca = pca.transform(X_test.toarray())\n",
    "                        \n",
    "\n",
    "# You should end up with each sample having just two features.\n",
    "assert_equal(X_train_pca.shape[1], 2)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=twenty_train.target, cmap='viridis', s=20)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA 2D Projection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape without PCA: {X_train.shape}, {X_test.shape}\")\n",
    "print(f\"Shape with PCA: {X_train_pca.shape}, {X_test_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3562197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_train_pca = pca.fit_transform(X_train.toarray())\n",
    "X_test_pca = pca.transform(X_test.toarray())\n",
    "\n",
    "# You should end up with each sample having just three features.\n",
    "assert_equal(X_train_pca.shape[1], 3)\n",
    "\n",
    "# Create a trace for the scatter plot\n",
    "trace = go.Scatter3d(\n",
    "    x=X_train_pca[:, 0],\n",
    "    y=X_train_pca[:, 1],\n",
    "    z=X_train_pca[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=twenty_train.target,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the layout for the plot\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='PC1'),\n",
    "        yaxis=dict(title='PC2'),\n",
    "        zaxis=dict(title='PC3')\n",
    "    ),\n",
    "    title='PCA 3D Projection'\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9db2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components=3)\n",
    "X_train_pca = pca.fit_transform(X_train.toarray())\n",
    "X_test_pca = pca.transform(X_test.toarray())\n",
    "\n",
    "# You should end up with each sample having just three features.\n",
    "assert_equal(X_train_pca.shape[1], 3)\n",
    "\n",
    "# Create a trace for the scatter plot\n",
    "trace = go.Scatter3d(\n",
    "    x=X_train_pca[:, 0],\n",
    "    y=X_train_pca[:, 1],\n",
    "    z=X_train_pca[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=twenty_train.target,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the layout for the plot\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='PC1'),\n",
    "        yaxis=dict(title='PC2'),\n",
    "        zaxis=dict(title='PC3')\n",
    "    ),\n",
    "    title='PCA 3D Projection'\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500bb518",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "499b77624b447c22c2e0fb9d6c3e9107",
     "grade": false,
     "grade_id": "cell-eb8ba9f77172acbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write the conclusions about the PCA in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359b745",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ef5265b7bfb404fef9fdbbd58ef9a53",
     "grade": true,
     "grade_id": "cell-690b22496863ff5d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "> There is a obvious split within the data. But because of the amount of points, there are a lot of wrong labelt points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e45cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "276d2984ea10ea5f636c702e7791820e",
     "grade": false,
     "grade_id": "cell-f297ce9badb1c019",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Extra \n",
    "If you are interested, use the parsimonious language models programmed in `https://github.com/larsmans/weighwords` to create a small language model for each class. Print the top 10-20 terms for each class. Do they make sense? Can you use these highly separating terms only for a classifier? Think of a simple implementation, and try it out.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8e432",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e17f5034f65b95bf10889e0ea1a90df5",
     "grade": false,
     "grade_id": "cell-586fbe838f592f30",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#WRITE YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
