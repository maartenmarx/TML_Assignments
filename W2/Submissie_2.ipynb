{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92336f22",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "privacy",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment Applied Machine Learning BSc IK \n",
    "\n",
    "## Notebook made by\n",
    "\n",
    "**Gebruik graag dit formaat**\n",
    "\n",
    "* Voor de namen:  voornaam rest van je naam, voornaam rest van je naam,....\n",
    "* je studentnummers: hetzelfde: scheidt met `,`\n",
    "* je emails: hetzelfde: scheidt met `,`\n",
    "* voor je groep: **alleen de hoofdletter** (iets als  `A` of `B` dus)\n",
    "\n",
    "__Namen__:Anoniem",
    "\n",
    "__Emails__:Anoniem",
    "\n",
    "__Student id__:Anoniem",
    "\n",
    "__Groep__:Anoniem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747951b",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* Een aantal opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. Dit zijn dus *NIET* alle tests, die komen er bij het graden nog bij.\n",
    "* Elke vraag is 1 punt waard, tenzij anders aangegeven. Soms is die punt onderverdeeld in deelpunten, maar niet altijd. \n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren\n",
    "\n",
    "## Na het inleveren!\n",
    "\n",
    "* Het gebeurt erg vaak dat mensen een \"leeg bestand\" inleveren. Vaak een andere versie van de opgave die nog ergens op je computer rondslingerde. Zonde van al je werk toch!\n",
    "* Dus, lever **minstens een half uur voor tijd in**. Download dan wat je hebt ingeleverd op Canvas. Geef het een andere naam om verwarring te voorkomen. En draai alle cellen, en bekijk het. Geen syntax fouten? Alle vragen gemaakt? Dan zit het vast wel goed, en hoef je niet in de zenuwen te zitten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45023bd1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b5e37f3720b0011335209f22450abff",
     "grade": false,
     "grade_id": "cell-aa5556336c90c946",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Applied Machine Learning W2\n",
    "\n",
    "This assignment tests the concepts that have been discussed in this week's lecture, and is divided in several (mostly) independent exercises. \n",
    "\n",
    "1. [Agglomerative Clustering](#agglo)\n",
    "2. [DBSCAN](#dbscan)\n",
    "3. [Evaluation](#elm)\n",
    "4. [Data Normalization](#normalization)\n",
    "5. [PCA](#pca)\n",
    "\n",
    "### TIP\n",
    "The second assignment, which asks you to implement the DBSCAN algorithm, can be a bit challenging, so don't forget about the other exercises! Just start working on another part of the assignment if you get stuck, and come back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41d40c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19d94f2dae632e363cb7229b0c8d1b8a",
     "grade": false,
     "grade_id": "imp",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from nose.tools import assert_count_equal, assert_equal\n",
    "from numpy.testing import *\n",
    "from sklearn import datasets\n",
    "# Please do not remove this: \n",
    "np.random.seed(31415)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea28b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0853037960cc21f5bab540a250c8717",
     "grade": false,
     "grade_id": "cell-5cfe8bcdbe386cfd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"agglo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16d6cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50f017fa3a57a280461b6e4e0cb33c2f",
     "grade": false,
     "grade_id": "agg",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 1 Agglomerative Clustering\n",
    "\n",
    "1. We defined the Ward criterion in class. Now you define from scratch, similar as we did with Ward, *average* and *complete*.\n",
    "2. Create the complete algrithm for bottom up agglomerative clustering from scratch, giving, just like in sklearn, the possibility to choose the linking function. We will help you by having you implement the parts needed as seperate functions.\n",
    "3. Use the Euclidean distance in your functions.\n",
    "3. Try out your code on the iris dataset.\n",
    "4. Compare it to sklearn. I mean the outcomes on the same input. \n",
    "5. Use the technique described in the book to create a dendogram.\n",
    "\n",
    "## Tips\n",
    "* explain what you are doing\n",
    "* test and test and test\n",
    "* refactor, refactor\n",
    "* Be proud. Don't despair if it takes hours to write a few lines of code.\n",
    "    * That is because Python is such a fantastic high level language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f27690",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1aca492bcbb7271b3ee314ddf60cd8d",
     "grade": false,
     "grade_id": "cell-3ea2cd4a29042123",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def average_linkage(cluster1, cluster2) -> np.float64:\n",
    "    n1, n2 = len(cluster1), len(cluster2)\n",
    "    sum_distances = 0\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            sum_distances += np.linalg.norm(cluster1[i] - cluster2[j])\n",
    "    distance = sum_distances / (n1 * n2)\n",
    "    return distance\n",
    "\n",
    "def complete_linkage(cluster1, cluster2) -> np.float64:\n",
    " \n",
    "    n1, n2 = len(cluster1), len(cluster2)\n",
    "    max_distance = 0\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            distance = np.linalg.norm(cluster1[i] - cluster2[j])\n",
    "            if distance > max_distance:\n",
    "                max_distance = distance\n",
    "    return max_distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0790b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e19824f318d83f364907e2553b50600",
     "grade": true,
     "grade_id": "cell-0b2ecbbc446d0351",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that you get the right output\n",
    "cluster_1 = np.random.rand(5, 2)\n",
    "cluster_2 = np.random.rand(10, 2)\n",
    "assert_equal(type(average_linkage(cluster_1, cluster_2)), np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95369b3b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e545b4924c30eb441968ea93d1ae1610",
     "grade": false,
     "grade_id": "cell-8ed320830e08b11b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After implementing the two linkage functions, you will implement a single step of the clustering algorithm, which will return the IDs of the clusters in the input that should be merged. As input you will have both the original input data, as well as a dictionary of clusters, where each key is an ID, and each value is a list of points belonging to that cluster. You can use fancy indexing from numpy to get the rows belonging to a cluster easily:\n",
    "\n",
    "```\n",
    "# suppose we have input data X\n",
    "clusters = {0: [0, 1, 2, 3], 4: [4, 6, 7]}\n",
    "# get the vectors of our first cluster\n",
    "cluster_vectors = X[clusters[0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa4ad6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc8027b72773d8030c2a13cb060cdad5",
     "grade": false,
     "grade_id": "cell-a5c39cdf5bd549be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def clustering_step(input_data, cluster_dict, linkage_type: str) -> tuple:\n",
    "    \n",
    "    assert linkage_type in ['average', 'complete']\n",
    "    cluster_ids = [None, None]\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    linkage_function = average_linkage if linkage_type == 'average' else complete_linkage\n",
    "    \n",
    "  \n",
    "    for id1 in cluster_dict:\n",
    "        for id2 in cluster_dict:\n",
    "            if id1 != id2:\n",
    "        \n",
    "                cluster1_vectors = input_data[cluster_dict[id1]]\n",
    "                cluster2_vectors = input_data[cluster_dict[id2]]\n",
    "                distance = linkage_function(cluster1_vectors, cluster2_vectors)\n",
    "                \n",
    "               \n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    cluster_ids = [id1, id2]\n",
    "    \n",
    "    return tuple(sorted(cluster_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb5783",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea2aabc53adc8451541df07a61b1bc6f",
     "grade": true,
     "grade_id": "cell-ea15f9c2e6d76953",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "input_array = np.array([\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [3, 3, 3]])\n",
    "clusters = {0: [0], 1: [1], 2: [2]}\n",
    "\n",
    "assert_equal(type(clustering_step(input_array, clusters, linkage_type='average')), tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae4f7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "681e0baf2b35069f7a230df3eb30b4c7",
     "grade": false,
     "grade_id": "cell-f43337b78b4cde61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In our last step, we will implement the complete algorithm for agglomerative clustering, using our previously created `clustering_step` and linkage functions. For the return value, return a list of labels where each label corresponds to the class of the instance. It doesn't matter if these values are not 0/1 as long as items in the same cluster share the same label. \n",
    "\n",
    "**TIP**\n",
    "You can delete key value pairs from dictionaries using the `del dictionary[key]` syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a61b7f0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b540791a37cc10893ad3097474351441",
     "grade": false,
     "grade_id": "cell-5524b767782337df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# in our version, we will define a number of clusters we want to find\n",
    "# you may also define this in sklearn for your comparison later\n",
    "def agglomerative_clustering(input_points, number_of_clusters: int, linkage_type: str):\n",
    "    assert linkage_type in ['average', 'complete']\n",
    "    \n",
    "\n",
    "    clusters = {i: [i] for i in range(len(input_points))}\n",
    "\n",
    "    # Look at the stopping condition, understand why?\n",
    "    while len(clusters) != number_of_clusters:\n",
    "        # Find the two clusters to merge\n",
    "        cluster_ids_to_merge = clustering_step(input_points, clusters, linkage_type)\n",
    "        \n",
    "        # Merge the clusters\n",
    "        merged_cluster = clusters[cluster_ids_to_merge[0]] + clusters[cluster_ids_to_merge[1]]\n",
    "        \n",
    "        # Remove old clusters and add the new merged cluster\n",
    "        del clusters[cluster_ids_to_merge[0]]\n",
    "        del clusters[cluster_ids_to_merge[1]]\n",
    "        clusters[min(cluster_ids_to_merge)] = merged_cluster\n",
    "    \n",
    "    # create the output array (we will do this for you)\n",
    "    output_labels = np.zeros((input_points.shape[0], 1))\n",
    "    for i, point_ids in enumerate(clusters.values()):\n",
    "        output_labels[point_ids] = i\n",
    "    \n",
    "    return output_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96d85b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8751bfa61fa74185fa315d6bd3d581d",
     "grade": true,
     "grade_id": "cell-790357a97ba5f07b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Did we actually create the right amount of clusters?\n",
    "samples = np.random.rand(10, 3)\n",
    "labels = agglomerative_clustering(samples, number_of_clusters=4, linkage_type=\"average\")\n",
    "assert_equal(len(np.unique(labels)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa072cd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8de534aefb9196e9822e264da26309f",
     "grade": true,
     "grade_id": "cell-daf2aae5565a776f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Loaddataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "\n",
    "number_of_clusters = 3\n",
    "linkage_type = 'average'\n",
    "\n",
    "custom_labels = agglomerative_clustering(X, number_of_clusters, linkage_type)\n",
    "\n",
    "sklearn_model = AgglomerativeClustering(n_clusters=number_of_clusters, linkage=linkage_type)\n",
    "sklearn_labels = sklearn_model.fit_predict(X)\n",
    "\n",
    "\n",
    "print(\"Custom agglomerative clustering labels:\")\n",
    "print(custom_labels.flatten())\n",
    "\n",
    "print(\"Sklearn agglomerative clustering labels:\")\n",
    "print(sklearn_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e73d2d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5cfea9e0553e7f8af566f0ffe9681d9",
     "grade": false,
     "grade_id": "cell-74b3f23a427ac8db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you have created the agglomerative algorithm, create the dendogram and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61b0bf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de38763f9ae732d8a87c894311c776ec",
     "grade": true,
     "grade_id": "cell-70688f9ad0bbf00e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def agglomerative_clustering_dendrogram(input_points, linkage_type: str):\n",
    "    assert linkage_type in ['average', 'complete']\n",
    "    \n",
    "    clusters = {i: [i] for i in range(len(input_points))}\n",
    "    merge_steps = []\n",
    "    new_cluster_id = len(input_points)\n",
    "\n",
    "    while len(clusters) != 1:\n",
    "        cluster_ids_to_merge = clustering_step(input_points, clusters, linkage_type)\n",
    "        \n",
    "        # Calculate the distance between the clusters to be merged\n",
    "        cluster1_vectors = input_points[clusters[cluster_ids_to_merge[0]]]\n",
    "        cluster2_vectors = input_points[clusters[cluster_ids_to_merge[1]]]\n",
    "        linkage_function = average_linkage if linkage_type == 'average' else complete_linkage\n",
    "        distance = linkage_function(cluster1_vectors, cluster2_vectors)\n",
    "\n",
    "        \n",
    "        merged_cluster = clusters[cluster_ids_to_merge[0]] + clusters[cluster_ids_to_merge[1]]\n",
    "        merge_steps.append([cluster_ids_to_merge[0], cluster_ids_to_merge[1], distance, len(merged_cluster)])\n",
    "        \n",
    " \n",
    "        del clusters[cluster_ids_to_merge[0]]\n",
    "        del clusters[cluster_ids_to_merge[1]]\n",
    "        clusters[new_cluster_id] = merged_cluster\n",
    "        new_cluster_id += 1\n",
    "    \n",
    "    return merge_steps\n",
    "\n",
    "\n",
    "linkage_type = 'average'\n",
    "dendrogram_data = agglomerative_clustering_dendrogram(X, linkage_type)\n",
    "\n",
    "# Plot  dendrogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram(dendrogram_data)\n",
    "plt.title(\"Dendrogram - Agglomerative Clustering\")\n",
    "plt.xlabel(\"Data points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af2b3c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "777aee9e63136c0c3ab45cd63e20aa21",
     "grade": false,
     "grade_id": "cell-2dc5be82f1c567be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"dbscan\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c640a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72e1a5a2f53e02ebe212a2b6a8d26e0c",
     "grade": false,
     "grade_id": "dbscan",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 2 DBscan\n",
    "\n",
    "In class and in section of the book 3.5.3 the DBScan algorithm was discussed.\n",
    "In this assignment we are going to implement the DBScan metric from scratch, working from first principles and using euclidean distance as the distance metric.\n",
    "\n",
    "**TIPS**\n",
    "- Implementing this from scratch is a challenge, but it should be doable, make sure you really understand/can explain the algorithm before you start coding, this should already help you when programming. Like before, we will also make you implement sub functions first to help you along.\n",
    "- Remember that in the algorithm we include the sample itself in the calculation of the distances and `min_samples`, you don't have to write separate logic to exlcude the point itself from the neighbour calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27992de1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e23d3c1b0cd9386dfe53db6bb06e81",
     "grade": false,
     "grade_id": "cell-a6ef76f7add38bfc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_neighbours(input_data, point_index, eps) -> list:\n",
    " \n",
    "    neighbors = []\n",
    "    point = input_data[point_index]\n",
    "\n",
    "    for i, other_point in enumerate(input_data):\n",
    "        distance = np.linalg.norm(point - other_point)\n",
    "        if distance <= eps:\n",
    "            neighbors.append(i)\n",
    "\n",
    "    return neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c3d55",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "227a28723d3539bad7d768bbcc388970",
     "grade": true,
     "grade_id": "cell-ce6bd3fd8c8a01db",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the output contains integers, which should correspond\n",
    "# to the indices of the neighbours\n",
    "test_data = np.random.rand(5, 3)\n",
    "assert_equal(type(get_neighbours(test_data, 0, eps=1)[0]), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f42af4e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e02877ab487c8044d18ef7b42f65333",
     "grade": false,
     "grade_id": "cell-ee48d6823dedb0d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After we have implemented the function to get all neighbours of a point for a specific point, we will write a function that, starting from a specific point, finds all neighbour points, and their neighbours points, and so on, until the cluster can no longer be expanded.\n",
    "\n",
    "## TIPS\n",
    " - In this function and the next, we will assume points that have a value of `-1` in the cluster are ure (currently) outliers, and points with a value of `-2` have not yet been examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bc55f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cd074839ad2dcc59345b30cb13b1a65",
     "grade": false,
     "grade_id": "cell-530e8aff991748e1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def expand(input_data, clusters, point, class_label, eps, min_samples):\n",
    "\n",
    "    neighbours = get_neighbours(input_data, point, eps)\n",
    "    clusters[point] = class_label\n",
    "\n",
    "    i = 0\n",
    "    while i < len(neighbours):\n",
    "        neighbour_point = neighbours[i]\n",
    "        point_cluster = clusters[neighbour_point]\n",
    "\n",
    "        if point_cluster == -2:  # unassigned point\n",
    "  \n",
    "            clusters[neighbour_point] = class_label\n",
    "\n",
    "            neighbour_neighbours = get_neighbours(input_data, neighbour_point, eps)\n",
    "\n",
    "          \n",
    "            if len(neighbour_neighbours) >= min_samples:\n",
    "                neighbours += neighbour_neighbours\n",
    "\n",
    "  \n",
    "        elif point_cluster == -1:\n",
    "            clusters[neighbour_point] = class_label\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b811cd8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f67771e1332793ac078418a69f180f3",
     "grade": false,
     "grade_id": "cell-0d1aed0778a65657",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the cell below we will put everything we made so far together and create the complete DBSCAN algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b771dfc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "622e048dd59e3930554d932596c165b7",
     "grade": true,
     "grade_id": "dbscana",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dbscan(input_data, eps, min_samples):\n",
    "    \"\"\"\n",
    "    Implements the complete dbscan algorithm using the helper functions we wrote before.\n",
    "    Takes as input the complete input matrix, the maximum distance neighbours can be apart, eps,\n",
    "    and the minimal number of samples that are neighbours from a point to let it be considered\n",
    "    a core point. Returns an array with for each item the class label of that item, where -1\n",
    "    indicates that the item is an outlier.\n",
    "    \"\"\"\n",
    "    current_class = 0\n",
    "    \n",
    "    clusters = [-2 for _ in range(input_data.shape[0])]\n",
    "\n",
    "    for i in range(0, input_data.shape[0]):\n",
    "        if clusters[i] == -2:  # unassigned point\n",
    "            neighbours = get_neighbours(input_data, i, eps)\n",
    "\n",
    "         \n",
    "            if len(neighbours) >= min_samples:\n",
    "                clusters = expand(input_data, clusters, i, current_class, eps, min_samples)\n",
    "                current_class += 1\n",
    "            else:\n",
    "                clusters[i] = -1  # outlier\n",
    "\n",
    "    return np.array(clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09b3b7",
   "metadata": {},
   "source": [
    "Compare the output of your algorithm to the output of sklearns `DBSCAN` in the cell below, explaing your tests in the comments of the cell\n",
    "\n",
    "### TIP\n",
    "\n",
    "- Have a look at `make_blobs` from the sklearn.datasets module, this is a nice function with which you can easily\n",
    "create some sample clusters to test your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b723a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23913fd67d8375bfb9be9f5720e28d96",
     "grade": true,
     "grade_id": "cell-6e4d46c5a1c0a696",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data\n",
    "n_samples = 300\n",
    "centers = [(0, 0), (5, 5), (10, 10)]\n",
    "X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=1, random_state=42)\n",
    "\n",
    "# iplementatie DBSCAN\n",
    "clusters = dbscan(X, eps=2, min_samples=5)\n",
    "print(\"mijn implementatie van DBSCAN:\")\n",
    "print(clusters)\n",
    "\n",
    "# Sklearn DBSCAN\n",
    "sklearn_dbscan = SklearnDBSCAN(eps=2, min_samples=5)\n",
    "sklearn_clusters = sklearn_dbscan.fit_predict(X)\n",
    "print(\"Sklearn's implementation van DBSCAN:\")\n",
    "print(sklearn_clusters)\n",
    "\n",
    "# Visualise\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis')\n",
    "ax[0].set_title(\"mijn implementatie van DBSCAN\")\n",
    "\n",
    "ax[1].scatter(X[:, 0], X[:, 1], c=sklearn_clusters, cmap='viridis')\n",
    "ax[1].set_title(\"Sklearn's implementation van DBSCAN\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589322e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30a039da03152fc6b38297ca1d9e73a2",
     "grade": false,
     "grade_id": "cell-8e15e345ff215048",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"elm\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c493c1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9906a1b86c2a6d5530644c97cb372ee",
     "grade": false,
     "grade_id": "elm",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 3 Evaluation\n",
    "\n",
    "We can evaluate a cluster algorithm against a gold standard labelling. The tricky part then is which cluster to assign to which label. Think of Iris, and you have clustered it into 3 groups. Then which group do you want to assign to which species?\n",
    "\n",
    "A smart idea called BCubed, and later improved to [ELM](https://dl.acm.org/doi/abs/10.1145/3539813.3545121) can be used to evaluate the quality of a clustering.\n",
    "\n",
    "* implement ELM\n",
    "* For this exercise, implement the F1 score, where you calculate F1 for each element and then average it over all elements\n",
    "* test it with a number of clusterings you made of the IRIs set and another own found set with gold standard.\n",
    " - If you managed to implement your own version DBSCAN you can use these to make clusters of the IRIS dataset, otherwise just use DBSCAN from `sklearn`.\n",
    "* Experiment with normalization, and see the effect.\n",
    "* You are not training anything, so you simply cluster the full set and use the same set as test set.\n",
    "\n",
    "* Next to the code, you write a small report in a markdown cell on your findings.\n",
    "\n",
    "**TIPS**:\n",
    " - The ELM algorithm operates on points, and compares the cluster a point is in in the\n",
    " gold standard with a cluster a point is in in the prediction. \n",
    " - Using sets is useful!\n",
    " - Start by creating a nested dict where each key is the index of a point, and each value is a dict with the indices of its neighbours, exluding the element itself. Do this for buth the gold standard and the predicted cluster.\n",
    "\n",
    "\n",
    "### Test dbscan's parameters\n",
    "\n",
    "* Using our ELM algorithm, try several options for the DBSCAN algorithm and report which values worked best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be4b3a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c71dd99b82fa93fb7d4750b9487551e",
     "grade": false,
     "grade_id": "elm1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ELM(gold_standard_labels, predicted_labels):\n",
    "    def create_cluster_dict(labels):\n",
    "        cluster_dict = {}\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in cluster_dict:\n",
    "                cluster_dict[label] = set()\n",
    "            cluster_dict[label].add(i)\n",
    "        return cluster_dict\n",
    "\n",
    "    gold_standard_dict = create_cluster_dict(gold_standard_labels)\n",
    "    predicted_dict = create_cluster_dict(predicted_labels)\n",
    "\n",
    "    def calculate_F1(gold_standard, predicted):\n",
    "        precision = len(gold_standard.intersection(predicted)) / len(predicted)\n",
    "        recall = len(gold_standard.intersection(predicted)) / len(gold_standard)\n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    num_points = len(gold_standard_labels)\n",
    "    score = []\n",
    "\n",
    "    for point in range(num_points):\n",
    "        gold_standard_cluster = gold_standard_dict[gold_standard_labels[point]]\n",
    "        predicted_cluster = predicted_dict[predicted_labels[point]]\n",
    "        point_F1 = calculate_F1(gold_standard_cluster, predicted_cluster)\n",
    "        score.append(point_F1)\n",
    "\n",
    "    return np.mean(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82602592",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bee2f3df6f8d47fa5f65ce004ef3d8bb",
     "grade": true,
     "grade_id": "cell-dc7be1edda2e5f0e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(ELM([0, 1, 2, 2], [0, 0, 0, 0])), np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f1e44",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "533ddd45c7daa245931c56267af09569",
     "grade": true,
     "grade_id": "elm2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Try different variations of DBSCAN, and evaluate using the ELM score we just defined\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "#normaliseren\n",
    "X_normalized = StandardScaler().fit_transform(X)\n",
    "\n",
    "parameter_combinations = [\n",
    "    (eps, min_samples)\n",
    "    for eps in np.arange(0.1, 1.1, 0.1)\n",
    "    for min_samples in range(1, 11)\n",
    "]\n",
    "\n",
    "best_params = None\n",
    "best_elm_score = -1\n",
    "\n",
    "for eps, min_samples in parameter_combinations:\n",
    "    dbscan = SklearnDBSCAN(eps=eps, min_samples=min_samples)\n",
    "    predicted_labels = dbscan.fit_predict(X_normalized)\n",
    "    elm_score = ELM(y, predicted_labels)\n",
    "    \n",
    "    if elm_score > best_elm_score:\n",
    "        best_elm_score = elm_score\n",
    "        best_params = (eps, min_samples)\n",
    "\n",
    "    print(f\"eps: {eps}, min_samples: {min_samples}, ELM score: {elm_score}\")\n",
    "\n",
    "print(f\"Best parameters: eps={best_params[0]}, min_samples={best_params[1]}, ELM score: {best_elm_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194f6b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b1060268a1f939dd9c12e02dd0b009b",
     "grade": false,
     "grade_id": "cell-dd21fc6122e9947d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"normalization\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc0d7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "802ad376b9b2f3b2ab284cc5fa698281",
     "grade": false,
     "grade_id": "pp",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Preprocessing and scaling section 3.2\n",
    "\n",
    "\n",
    "1. Download iris data, and Z-transform it using your own code.\n",
    "    2. Give a convincing check/test(s) that it worked.\n",
    "2. Similary for the `min-max` scalar and sklearn's `normalizer`.\n",
    "    * Of course you also describe in good English and with a math formula what this scaler is doing.\n",
    "3. Figure out how sklearn's *RobustScaler* works, and implement it using your own code. You can use numpy to get the 1st and 3rd quartile.\n",
    "4. Test all your code on iris and at least one other dataset.\n",
    "    * Make your own tests which really test the math.\n",
    "    * Run the same ytransformation in sklearn and use something like `assert_array_almost_equal` to test that you are close enough to sklearn's.\n",
    "\n",
    "Answer both in code and in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807cf58",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e16f6db51c5db571544e800f17c02f2d",
     "grade": false,
     "grade_id": "pp1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "iris_X, iris_y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "def z_transform(input_data):\n",
    "    mean = np.mean(input_data, axis=0)\n",
    "    std = np.std(input_data, axis=0)\n",
    "    output_data = (input_data - mean) / std\n",
    "    return output_data\n",
    "\n",
    "\n",
    "def my_normalize(input_data):\n",
    "    norms = np.linalg.norm(input_data, axis=1)[:, np.newaxis]\n",
    "    output_data = input_data / norms\n",
    "    return output_data\n",
    "\n",
    "    \n",
    "def min_max(input_data, min_val=0, max_val=1):\n",
    "    min_input = np.min(input_data, axis=0)\n",
    "    max_input = np.max(input_data, axis=0)\n",
    "    output_data = (input_data - min_input) / (max_input - min_input) * (max_val - min_val) + min_val\n",
    "    return output_data\n",
    "\n",
    "def robust_scaler(input_data):\n",
    "    medians = np.median(input_data, axis=0)\n",
    "    q1, q3 = np.percentile(input_data, [25, 75], axis=0)\n",
    "    iqr = q3 - q1\n",
    "    output_data = (input_data - medians) / iqr\n",
    "    return output_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Z-transform\n",
    "z_transformed = z_transform(iris_X)\n",
    "sklearn_z_transformed = StandardScaler().fit_transform(iris_X)\n",
    "assert_array_almost_equal(z_transformed, sklearn_z_transformed)\n",
    "\n",
    "# Min-max scaling\n",
    "min_max_transformed = min_max(iris_X)\n",
    "sklearn_min_max_transformed = MinMaxScaler().fit_transform(iris_X)\n",
    "assert_array_almost_equal(min_max_transformed, sklearn_min_max_transformed)\n",
    "\n",
    "# Normalization\n",
    "normalized = my_normalize(iris_X)\n",
    "sklearn_normalized = Normalizer().fit_transform(iris_X)\n",
    "assert_array_almost_equal(normalized, sklearn_normalized)\n",
    "\n",
    "# Robust scaling\n",
    "robust_scaled = robust_scaler(iris_X)\n",
    "sklearn_robust_scaled = RobustScaler().fit_transform(iris_X)\n",
    "assert_array_almost_equal(robust_scaled, sklearn_robust_scaled)\n",
    "\n",
    "# Original iris data\n",
    "plt.scatter(iris_X[:, 0], iris_X[:, 1], c=iris_y)\n",
    "plt.title(\"Original iris data\")\n",
    "plt.xlabel(\"Sepal length\")\n",
    "plt.ylabel(\"Sepal width\")\n",
    "plt.show()\n",
    "\n",
    "# Z-transformed iris data\n",
    "plt.scatter(z_transformed[:, 0], z_transformed[:, 1], c=iris_y)\n",
    "plt.title(\"Z-transformed iris data\")\n",
    "plt.xlabel(\"Sepal length (z-transformed)\")\n",
    "plt.ylabel(\"Sepal width (z-transformed)\")\n",
    "plt.show()\n",
    "\n",
    "# Min-max scaled iris data\n",
    "plt.scatter(min_max_transformed[:, 0], min_max_transformed[:, 1], c=iris_y)\n",
    "plt.title(\"Min-max scaled iris data\")\n",
    "plt.xlabel(\"Sepal length (min-max scaled)\")\n",
    "plt.ylabel(\"Sepal width (min-max scaled)\")\n",
    "plt.show()\n",
    "\n",
    "# Normalized iris data\n",
    "plt.scatter(normalized[:, 0], normalized[:, 1], c=iris_y)\n",
    "plt.title(\"Normalized iris data\")\n",
    "plt.xlabel(\"Sepal length (normalized)\")\n",
    "plt.ylabel(\"Sepal width (normalized)\")\n",
    "plt.show()\n",
    "\n",
    "# Robust scaled iris data\n",
    "plt.scatter(robust_scaled[:, 0], robust_scaled[:, 1], c=iris_y)\n",
    "plt.title(\"Robust scaled iris data\")\n",
    "plt.xlabel(\"Sepal length (robust scaled)\")\n",
    "plt.ylabel(\"Sepal width (robust scaled)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346049a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f1beeaf8e13a82f5b6f50fa09ebd8c1",
     "grade": false,
     "grade_id": "cell-f493f8f905f7c1b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Briefly describe for each scaler what they do (and give the formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a627eb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "938200fa7dc00cb3f56b26859f80615b",
     "grade": true,
     "grade_id": "cell-b707fe459099e51b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Z-score or StandardScaler: This scaler adjusts the data's mean and standard deviation to 0 and 1, respectively. The calculation is (x - mean) / std, where x is the data point, mean the data mean, and std the data standard deviation.\n",
    "This scaler, known as a min-max scaler, adjusts the data so that it is between a defined minimum and maximum value. The equation is (x - min) / (max - min) * (max_val - min_val) + min_val, where x is the data point, min and max are the data's minimum and maximum values, and min_val and max_val are the user-specified minimum and maximum values.\n",
    "Normalization, is the process of scaling data so that each data point has a unit norm. The formula is x / ||x||, where ||x|| is the L2-norm of x and x is the data point.\n",
    "robust scaler: This scaler increases the data's resistance to outliers by scalating it using the interquartile range (IQR) rather than the standard deviation. (x - median) / IQR is the formula, where x is the data point, median is the data median, and IQR is the data's interquartile range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029b801",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d1413a56149c2decce443ee4feaf225",
     "grade": false,
     "grade_id": "cell-961f5e0a66bd6859",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For each of the functions you implemented above, write tests in the cell below to make sure your scaling is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6de4b1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e82fbed452ad2d759d09f86f3e2393ea",
     "grade": true,
     "grade_id": "cell-f92d1ebc3600c4b4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Z-transform\n",
    "z_transformed = z_transform(iris_X)\n",
    "sklearn_z_transformed = StandardScaler().fit_transform(iris_X)\n",
    "assert_array_almost_equal(z_transformed, sklearn_z_transformed)\n",
    "plt.scatter(z_transformed[:, 0], z_transformed[:, 1], c=iris_y)\n",
    "plt.title(\"Z-transformed Iris Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Min-max scaling\n",
    "min_max_transformed = min_max(iris_X)\n",
    "sklearn_min_max_transformed = MinMaxScaler().fit_transform(iris_X)\n",
    "assert_array_almost_equal(min_max_transformed, sklearn_min_max_transformed)\n",
    "plt.scatter(min_max_transformed[:, 0], min_max_transformed[:, 1], c=iris_y)\n",
    "plt.title(\"Min-max Scaled Iris Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Normalization\n",
    "normalized = my_normalize(iris_X)\n",
    "sklearn_normalized = Normalizer().fit_transform(iris_X)\n",
    "assert_array_almost_equal(normalized, sklearn_normalized)\n",
    "plt.scatter(normalized[:, 0], normalized[:, 1], c=iris_y)\n",
    "plt.title(\"Normalized Iris Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Robust scaling\n",
    "robust_scaled = robust_scaler(iris_X)\n",
    "sklearn_robust_scaled = RobustScaler().fit_transform(iris_X)\n",
    "assert_array_almost_equal(robust_scaled, sklearn_robust_scaled)\n",
    "plt.scatter(robust_scaled[:, 0], robust_scaled[:, 1], c=iris_y)\n",
    "plt.title(\"Robustly Scaled Iris Dataset\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a0680",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b4670721462ddb2b2f4631ac5e274a7",
     "grade": false,
     "grade_id": "cell-06b3e7850304b90f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"pca\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d44f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f0dbeb760f236efdcc65798aa2711d4",
     "grade": false,
     "grade_id": "pca",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# PCA\n",
    "\n",
    "### inspect\n",
    "* get the sklearn digit dataset.\n",
    "* transform it to 2 dimensions using PCA, you are allowed to use the `sklearn` function for this exercise.\n",
    "* plot it in 2 dimensions using seaborn and use the `hue` parameter to see how much the digits are separated.\n",
    "* Does it make sense? Do you see similar \"confusions/overlap\" as seen before in the book between certain digits?\n",
    " - Play around with the color palete to ensure that the different classes have clearly different colours.\n",
    "\n",
    "\n",
    "### cluster\n",
    "\n",
    "Run K-means on this reduced data (2 instead of 64 dimensions!), also run K-means on the original data. Then compare the two using the ELM-metric. For this experiment you are allowed to use the K means algorithm from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89eddce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b23f55f7df5b6de8ef18ffe0be5d6338",
     "grade": false,
     "grade_id": "pca1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "digits_X, digits_y = load_digits(return_X_y=True)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "digits_pca = pca.fit_transform(digits_X)\n",
    "\n",
    "\n",
    "sns.scatterplot(x=digits_pca[:,0], y=digits_pca[:,1], hue=digits_y, palette='Set2')\n",
    "\n",
    "kmeans_pca = KMeans(n_clusters=10, random_state=42).fit(digits_pca)\n",
    "labels_pca = kmeans_pca.labels_\n",
    "\n",
    "\n",
    "elm_pca = adjusted_rand_score(digits_y, labels_pca)\n",
    "print(\"ELM-metric for PCA-reduced data:\", elm_pca)\n",
    "\n",
    "kmeans_orig = KMeans(n_clusters=10, random_state=42).fit(digits_X)\n",
    "labels_orig = kmeans_orig.labels_\n",
    "elm_orig = adjusted_rand_score(digits_y, labels_orig)\n",
    "print(\"ELM-metric for original data:\", elm_orig)\n",
    "\n",
    "if elm_pca > elm_orig:\n",
    "    print(\"K-means op PCA-reduced data is beter.\")\n",
    "else:\n",
    "    print(\"K-means op origineel data is beter.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b78c6d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a457abf9743a62a070d863052850b21e",
     "grade": true,
     "grade_id": "cell-dbb42e9722e026d4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(perform_pca(digits_X).shape[-1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f746d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eeab35b70dfe24810aba71074eaf45ef",
     "grade": false,
     "grade_id": "cell-b98baba6d68fee4b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement your plotting here\n",
    "import matplotlib.pyplot as plt\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e6447",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7bb11ba88acbf081f7adf8e54970a9c",
     "grade": false,
     "grade_id": "cell-1b2f2dab6930142d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does your plot make sense? Do you see similar overlaps as in the book?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f0961",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02eca7d0c0c06674663559214eb5149a",
     "grade": true,
     "grade_id": "pca2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d17ba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bd71c3cd0a849b2a2f6b3fc39813d06",
     "grade": true,
     "grade_id": "cell-7438942e6e2adfa8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compare K means on both the reduced and unreduced data and evaluation using ELM.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff654e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
