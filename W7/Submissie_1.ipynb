{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f0f2d1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "privacy",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment Applied Machine Learning BSc IK \n",
    "\n",
    "## Notebook made by\n",
    "\n",
    "**Gebruik graag dit formaat**\n",
    "\n",
    "* Voor de namen:  voornaam rest van je naam, voornaam rest van je naam,....\n",
    "* je studentnummers: hetzelfde: scheidt met `,`\n",
    "* je emails: hetzelfde: scheidt met `,`\n",
    "* voor je groep: **alleen de hoofdletter** (iets als  `A` of `B` dus)\n",
    "\n",
    "__Namen__:Anoniem",
    "\n",
    "__Emails__:Anoniem",
    "\n",
    "__Student id__:Anoniem",
    "\n",
    "__Groep__:Anoniem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa024ae",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* Een aantal opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. Dit zijn dus *NIET* alle tests, die komen er bij het graden nog bij.\n",
    "* Elke vraag is 1 punt waard, tenzij anders aangegeven. Soms is die punt onderverdeeld in deelpunten, maar niet altijd. \n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren\n",
    "\n",
    "## Na het inleveren!\n",
    "\n",
    "* Het gebeurt erg vaak dat mensen een \"leeg bestand\" inleveren. Vaak een andere versie van de opgave die nog ergens op je computer rondslingerde. Zonde van al je werk toch!\n",
    "* Dus, lever **minstens een half uur voor tijd in**. Download dan wat je hebt ingeleverd op Canvas. Geef het een andere naam om verwarring te voorkomen. En draai alle cellen, en bekijk het. Geen syntax fouten? Alle vragen gemaakt? Dan zit het vast wel goed, en hoef je niet in de zenuwen te zitten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ed3e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a429d9fa49cff50e9c34ab2efab0bf6",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Applied Machine Learning W7\n",
    "\n",
    "In the exercises of this week's assignment we are going to be focussing on Multilayer Perceptrons, and investigate what makes them tick, taking an in-depth look at activation functions, and on how we can apply these models to an actual real world task. Apart from this we will also be recapping on some of the data preprocessing techniques that we have touched on before, taking a more in-depth look at missing values and categorical variables.\n",
    "\n",
    "For this assignment there is a total number of 15 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc0a944",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3012c4b6ce4f1b8cecd3bb6241eb3b14",
     "grade": false,
     "grade_id": "index",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Index\n",
    "\n",
    "1. [Activation Functions](#activation)\n",
    "2. [Number of Parameters in Neural Nets](#params)\n",
    "3. [Categorical Variables](#categorical)\n",
    "4. [Missing Values](#missing_values)\n",
    "5. [MLPs in Action: Page Stream Segmentation](#mlp_pss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa17ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "900114fadc429811e012d5979eecac17",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sklearn\n",
    "from scipy.stats import mode\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import datasets\n",
    "# testing\n",
    "from nose.tools import assert_count_equal, assert_equal, assert_almost_equals\n",
    "from numpy.testing import *\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Please do not remove this: \n",
    "np.random.seed(31415)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef5c9e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbb4e354acb38f8064f4fec53324e321",
     "grade": false,
     "grade_id": "act",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<h1><a id=\"activation\">Plot activation functions</a></h1>\n",
    "\n",
    "In this first exercise we we will be taking a look at activation functions.\n",
    "\n",
    "Consider the code below which plots 2 activation functions, tanh and relu, and complete the exercises below.\n",
    " \n",
    " 1. Complete the sigmoid function given below, which should work on an input array `x`, you can use the `np.exp` function for this.\n",
    " 2. We are going to plot this data using Pandas.\n",
    "     * Create a pandas dataframe with index `line` and three columns `['sigmoid', 'tanh', 'relu']` (in this order), for the three activation functions.\n",
    "     * Plot the three activation functions from the dataframe with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712d571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4886389948a655cafe82a691c753a227",
     "grade": false,
     "grade_id": "actc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We are plotting activation functions for the line below\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "line = np.linspace(-3, 3, 100)\n",
    "plt.plot(line, tanh(line), label=\"tanh\")\n",
    "plt.plot(line, relu(line), label=\"relu\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"relu(x), tanh(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170ca51",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9cb59673fcd9487efd373646be16b38",
     "grade": false,
     "grade_id": "sigm1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy import exp\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "    Function that given an input array x computes the sigmoid function on\n",
    "    each element in x, the returned results should be a numpy array\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f08cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4a9eac878ad18f15ded47bbd8e7648d",
     "grade": true,
     "grade_id": "sigm1t",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check for the return type\n",
    "assert_equal(type(sigmoid(np.array([0.5, 1.2, 1.7]))), np.ndarray)\n",
    "# Also check for the correct shape, output shape should be same as the input shape\n",
    "assert_equal(np.array([0.5, 1.2, 1.7]).shape, sigmoid(np.array([0.5, 1.2, 1.7])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc955582",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6912b4a4c564f75f2a1514bf38f1ad14",
     "grade": false,
     "grade_id": "acta",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Values for which we do the plot\n",
    "line = np.linspace(-3, 3, 100)\n",
    "\n",
    "D= pd.DataFrame(index=line, columns=['sigmoid', 'tanh', 'relu'])\n",
    "D['sigmoid'] = sigmoid(line)\n",
    "D['tanh'] = tanh(line)\n",
    "D['relu'] = relu(line)\n",
    "\n",
    "D.plot()\n",
    "D.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164ced5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee2fd970271bd3a91dc3703ed1eb48de",
     "grade": true,
     "grade_id": "ACTT",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# D must have these columns, in this order, we also check that the index is correct\n",
    "assert_equal(type(D), pd.DataFrame)\n",
    "assert_array_equal(D.columns,['sigmoid', 'tanh', 'relu'])\n",
    "assert_array_equal(D.index, line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7c12c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0d7802c8d5ea84026d3eb7eb70f0b9d",
     "grade": false,
     "grade_id": "np",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<h1><a id=\"params\">Number of parameters</a></h1>\n",
    "\n",
    "The book has this computation of the number of parammeters in an MLP. But they forgot something.\n",
    "\n",
    "* What?\n",
    "* Explain how you would compute the correct amount.\n",
    "* Compute again and answer in the `correct_number_of_parameters` variable.\n",
    "\n",
    ">A helpful measure when thinking about the model complexity of a neural network is the number of weights or coefficients that are learned. If you have a binary classifica‚Äê tion dataset with 100 features, and you have 100 hidden units, then there are 100 * 100 = 10,000 weights between the input and the first hidden layer. There are also 100 * 1 = 100 weights between the hidden layer and the output layer, for a total of around 10,100 weights. [page 120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79a428",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c70d9abfbe5ceb5e6e70ed8880371e53",
     "grade": true,
     "grade_id": "np1a",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "100*100 = 10000\n",
    "Dan 100 * 1 = 100\n",
    "Maar er zijn 100 hidden units die niet erbij opgeteld zijn als biases en ook nog de 1 van de ouput.\n",
    "Daarom 10201."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd93bde",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26ac722a5d660da1bcefd5747a9793a6",
     "grade": false,
     "grade_id": "np1b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "correct_number_of_parameters= 10201 # change to your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6cf806",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40e6f9a393d1fcd053eba19f9ca52234",
     "grade": true,
     "grade_id": "np1bt",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(correct_number_of_parameters, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603dc88",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd2d49201f511bc776e66b1f516f6c0a",
     "grade": false,
     "grade_id": "np2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Larger Neural Networks\n",
    "\n",
    "1. How many parameters must an MLP with 33 input nodes and 15 hidden layers each having 78 nodes learn? (variable `v15_78`)\n",
    "2. Give all MLP architectures with exactly N parameters to learn (don't forget the biases!), you don't have to repeat different permutations of the same network.\n",
    "    * You may choose N, but hey, come on, don't choose it too childish eh. Like 1 or so, or 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60e382",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0007cec7591114307c5757fba6a17453",
     "grade": false,
     "grade_id": "np2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "input_nodes = 33\n",
    "hidden_layers = 15\n",
    "hidden_nodes = 78\n",
    "v15_78 = 0\n",
    "\n",
    "for i in range (hidden_layers):\n",
    "    v15_78 += input_nodes * hidden_nodes\n",
    "    v15_78 += hidden_nodes\n",
    "\n",
    "v15_78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3a835",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d837f1ac95d84aa846eb6eb14caa632",
     "grade": true,
     "grade_id": "np2at",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(v15_78, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92123991",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "923fba279a124652eb3692c999cedc6e",
     "grade": true,
     "grade_id": "np3t",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Dus 39780 parameters in een MPL met 33 input nodes, 15 hidden layers en 78 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e0ed3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c0af65572dd6ea264f4f384d8ec08ed",
     "grade": false,
     "grade_id": "hot",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<h1><a id=\"categorical\">Categorical variables</a></h1>\n",
    "\n",
    "1. What is one hot encoding? Explain and give a clear example.\n",
    "2. Suppose you have a dataset that has two features that are both categorical `var1` and `var2` (so the dataframe with the train features has two columns) with `var1` having `3` possible values and `var2` having `5` possible values, what is the dimension of the feature vector if we one-hot encoded both variables? Give your answer as the value of the `one_hot_encoded_size_var1_var2`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21383c77",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdbe7cf2dc6dcb0f4e82c7283ba8f636",
     "grade": true,
     "grade_id": "hota",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Bij one hot encoding worden categorische gezet in binair, voor gevallen waarbij het gebruik van getallen nodig is als input. Stel je hebt een dataset met namen en erachter de geslacht: 'man' of 'vrouw'. In de nieuwe dataset zou er kolommen 'man' en 'vrouw' zijn en dan een 1 bij het geslacht dat diegene is en een 0 als dit niet het geslacht van deze persoon is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b7de8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d432c981d2c7ee5605f01f276db1875",
     "grade": false,
     "grade_id": "hotb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_size_var1_var2 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a08754",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e331e32ce1188f1f2dfe20e1ec285ee",
     "grade": true,
     "grade_id": "hotbed",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(one_hot_encoded_size_var1_var2), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770aa2ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd5e90eea43604d0056572813943472e",
     "grade": false,
     "grade_id": "mv",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<h1><a id=\"missing_values\">Missing values</a></h1>\n",
    "\n",
    "The [DS handbook](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb) contains a section on missing data, but our own  sklearn book doesn't. Read up on it. The keywords are: missing values and imputation. It is important to remove missing values from our data, as many operations will not work with this (think of calculating cosine similarity with vectors containing NaNs in K nearest neighbour).\n",
    "\n",
    "We have already experimented with this in previous notebooks, and now we will do the same things also using pandas and numpy, investigating the differences between removing NaNs in the different libraries.\n",
    "\n",
    "1. What would you do with rows in your training set which have a few missing values?\n",
    "    * Motivate and explain. Give pros and cons. And what you can do in which situation.\n",
    "     \n",
    "2. Now you want to *repair* your X. How? Of course, you repair every column seperately.\n",
    "    \n",
    "    * With a categorical variable: how can you repair that wisely? Motivate!\n",
    "    \n",
    "    * With a numeric variable: how can you repair that wisely? Motivate! Did you use a hidden assumption about your data?\n",
    "    * What if your data is lognormal distributed (like income, or number of received likes). What would be your best choice?\n",
    "    \n",
    "3. Make a test in pandas by which you find out which columns contain missing data, by completing the `number_of_missing_values` function. Return a dictionary where each key-value pair is the column name as a string and the values is the number of missing values in that column.\n",
    "4. We have given you a small example array `data_with_nan`, which contains the `age` and `embark_town` columns from the titanic dataset, and your task is to repair it in `pandas`, `numpy` and `sklearn`, using the median for `age` and the mode for `embark_town`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a4c9e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c8fcd5b3def9c116888d92f5ac11274",
     "grade": false,
     "grade_id": "cell-5db0edd1dd51c663",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the markdown cell below, explain what do do when your data contains a few NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148a0f4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e3de1eaebe4a06e26b5e28e86560893",
     "grade": true,
     "grade_id": "mva",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Mean of mode of mediaan: gemiste waardes kunnen vervangen worden en gebruikt worden. Er is geen data dat niet gebruikt wordt. De waarden kunnen niet representatief zijn vanwege bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55716ba7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9517b43dddfee18db30e204177ed7ee1",
     "grade": false,
     "grade_id": "cell-0069bc44a40ee79c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And in the cell below, explain what you would to to repair them, both for categorical and continuous features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be480632",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4827ea9b9d8a9d2e2c0ef5f9a2977d4",
     "grade": true,
     "grade_id": "mva1b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Categorisch: De modus van de kolom van de Nan waarde, aangezien het de meestvoorkomende waarde vervangd. \n",
    "\n",
    "Continuous: De mean van de kolom van de Nan waarde, aangezien het de invloed van outliers minder sterk maakt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e5b4e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3fa04a7729387944fa164f16e3dd19f",
     "grade": false,
     "grade_id": "mva2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def number_of_missing_values(dataframe) -> dict:\n",
    "    missing_values = {} # fill this dict with key values pairs where the key is the column and the value is the number if nan values in the column\n",
    "\n",
    "    for column in dataframe.columns:\n",
    "        nan = dataframe[column].isna().sum()\n",
    "        missing_values[column] = nan\n",
    "    \n",
    "    return missing_values\n",
    "\n",
    "\n",
    "example_dataframe = pd.DataFrame({'a': [0, 1, np.nan], 'b': [1, np.nan, 4], 'c': [np.nan, np.nan, 3], 'd': [13,  9, 2]})\n",
    "number_of_missing_values(example_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e119b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0a1ce4eeea38be389d796135e473e3a",
     "grade": true,
     "grade_id": "mva2t",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check that we get the right output from your function\n",
    "test_df = pd.DataFrame({'a': [0, 1], 'b': [1, np.nan]})\n",
    "assert_equal(type(number_of_missing_values(test_df)), dict)\n",
    "# do we get the right dict size?\n",
    "assert_equal(len(number_of_missing_values(test_df)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74979a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "280e1e2266bc8118d4c9fa7464fb55b3",
     "grade": false,
     "grade_id": "cell-d286c3d4d1431229",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this next exercise, remove the nan values from the `titanic` dataframe using `pandas`, `numpy` and `sklearn` in the corresponding code cells, where you use the median for the `age` column, and the mode for the `embark_town` column. In all case, you are allowed to use `fillna` from Pandas to fill the NaN values once you calculated the right values to replace them with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20595a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5495154dd04d9250e9baf8761e09aa4",
     "grade": false,
     "grade_id": "mva3pandas",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First with pandas\n",
    "import seaborn as sns\n",
    "# We will work on two columns of the titanic dataset\n",
    "data_with_nan = sns.load_dataset('titanic')[['age', 'embark_town']]\n",
    "\n",
    "data_with_nan['age'].fillna(data_with_nan['age'].median(), inplace=True)\n",
    "data_with_nan['embark_town'].fillna(data_with_nan['embark_town'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50b92f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "063ae846a4607fec63d19d99b2f77a57",
     "grade": true,
     "grade_id": "mva3pandast",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(data_with_nan['age'].isna().sum(), 0)\n",
    "assert_equal(data_with_nan['embark_town'].isna().sum(), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff59e9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "195a91f1af43238ed4bf3fe682dd37b0",
     "grade": false,
     "grade_id": "mva3numpy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now with Numpy\n",
    "# TIP: scipy.mode will no longer work for non-numeric arrays, you \n",
    "# can use a workaround with np.unique and return_counts=True, where you have to \n",
    "# skip the nan values for the mode calculation\n",
    "data_with_nan = sns.load_dataset('titanic')[['age', 'embark_town']]\n",
    "\n",
    "data_with_nan['age'] = data_with_nan['age'] # replace with you answer\n",
    "data_with_nan['embark_town'] = data_with_nan['embark_town'] # replace with you answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9c9129",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f0b3c87b38d40d79bad6a447cd94aaa",
     "grade": true,
     "grade_id": "mva3numpyt",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(data_with_nan['age'].isna().sum(), 0)\n",
    "assert_equal(data_with_nan['embark_town'].isna().sum(), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99269edf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b64121613f4c36210da48af807b97475",
     "grade": false,
     "grade_id": "mva3sklearn",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# And finally with sklearn\n",
    "# If you get a ValueError with the SimpleImputer and the 'most_frequent' parameter\n",
    "# make sure the output of the imputer is a 1D array (you can use np.ravel to enfore this)\n",
    "# \n",
    "from sklearn.impute import SimpleImputer\n",
    "data_with_nan = sns.load_dataset('titanic')[['age', 'embark_town']]\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data_with_nan['age'] = imputer.fit_transform(data_with_nan[['age']])\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_with_nan['embark_town'] = imputer.fit_transform(data_with_nan[['embark_town']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b7fea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c128cc834a9f3768e58f5b8357f917cb",
     "grade": true,
     "grade_id": "mva3sklearnt",
     "locked": true,
     "points": 0.34,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(data_with_nan['age'].isna().sum(), 0)\n",
    "#assert_equal(data_with_nan['embark_town'].isna().sum(), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed5e9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69f5c68652f1e17a6cec748ee58335d7",
     "grade": false,
     "grade_id": "mlp",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<h1><a id=\"mlp_pss\">Page stream segmentation with an MLP</a></h1>\n",
    "\n",
    "Now that we have taken a look at the Multilayer Perceptron, we are going to use it to tackle a real world classification task.\n",
    "\n",
    "In this excercise we are going to tackle a classification problem called **Page Stream Segmentation**. In this task the goal is to determine, based on text of pages, which page starts a new document, which is for example useful in cases where multiple documents are scanned in consecutively and they need to be separated. (If you want to read more about this, check out our paper on this where we also explain the dataset we introduced.)\n",
    "\n",
    "Text is tricky to work with in MLPs, as there is no fixed number of features, the text length is variable. As you probably know, an MLP expects the input to have a fixed number of variables. For this exercise we are not going to worry about this too much, and you are allowed to use the `CountVectorizer` method from Sklearn, which will create fixed-size vectors for each page by counting words.  \n",
    "\n",
    "We are going to conduct a complete ML experiment: loading and cleaning data, splitting the data in train, validation and test, run a baseline, develop our own method, and evaluate the results. To help you along, the excercise is split up into several parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1837b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc79161e58f8304d1ab48f0b6e17bef2",
     "grade": false,
     "grade_id": "pss1intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Reading and cleaning the data\n",
    "\n",
    "First, read in the data, and clean it up so that it contains no more NaN values (you can just use the empty string to fill them). Also make sure the the `label` column is of the `int` type. As always, use the `loadfile` function below to get the right path for the dataloading, this is essential for autograding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85bd9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7721e5ccc43c9cfda9775ff7343d0f3e",
     "grade": false,
     "grade_id": "pssload",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def loadfile():\n",
    "    if 'data.csv' in os.listdir():\n",
    "        return 'data.csv'\n",
    "    elif os.path.exists('../../data/Week7/'):\n",
    "        return '../../data/Week7/data.csv'\n",
    "    elif os.path.exists('../../../data/Week7/'):\n",
    "        return '../../../data/Week7/data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0a5e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcaabc10fa05d9cfb5680ba8c6852362",
     "grade": false,
     "grade_id": "pss1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = None # Load in the dataframe here\n",
    "\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60d972",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "640332e5594a647a6a2ce2b3cc8547b0",
     "grade": true,
     "grade_id": "pss1t",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(data.shape[0], 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b260a87",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f198f09c1d959a2ed3187fe9f75e5589",
     "grade": false,
     "grade_id": "cell-494a1b2480cecdaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Splitting the data and converting to vectors\n",
    "\n",
    "Split the data into train, validation and tests splits, with a 60, 20, 20 percent distribution over these sets, assuring that the data is stratified. You are allowed to use the `train_test_split` function from sklearn here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2049df4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89ed5247c12f208d3fde07bce7127fa4",
     "grade": false,
     "grade_id": "pss2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = None, None, None, None, None, None\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821daca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45c40362edbfd2901c4f20f3ddbaff47",
     "grade": true,
     "grade_id": "pss2t",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(X_train.shape[0], 1500)\n",
    "assert_equal(X_val.shape[0], 500)\n",
    "assert_equal(X_test.shape[0], 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb855e91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7515f6f6d5fe7912ead3c7de3e22a1be",
     "grade": false,
     "grade_id": "cell-69b485ed6af664a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the cell below convert the text of the pages into vectors containing words counts. Use the `CountVectorizer` method for this. Take a look at the documentation for the Countvectorizer, how should you use it? This should feel familiar to other sklearn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b83f93",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "666091bbaa57c53616f6374ecc83f737",
     "grade": false,
     "grade_id": "pss2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement your text conversion step here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "word_vectorizer = CountVectorizer(ngram_range=(2, 3))\n",
    "\n",
    "X_train_vectors = None\n",
    "X_val_vectors = None\n",
    "X_test_vectors = None\n",
    "\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ac844",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c29c9d8ff8c083ca57209683283f76b8",
     "grade": true,
     "grade_id": "pss2bt",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# if you did this correctly then all three arrays should have the same number of features\n",
    "assert X_train_vectors.shape[1] == X_val_vectors.shape[1] == X_test_vectors.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a7b798",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca75a42cc430dbaa9bfb71594518eb58",
     "grade": false,
     "grade_id": "cell-8f59b02a2d1b6f0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Classification\n",
    "\n",
    "Now that we have read in and prepared the data we will implement the baseline model and our own model. \n",
    "1. For the baseline model you MUST implement a single node MLP network (of course it isnt really Multilayer now) with a sigmoid activation fuction (in essence logistic regression). Make this model in the cell below and run the model on the test set we just created.\n",
    "2. For your own finetuned model you must implement an MLP network, but the exact size, activation function and other parameters are up to you, you should experiment with these and find the one that works best.\n",
    "3. Try to be smart with this, the dataset is quite large so a simple grid search over many values will take a lot of time. You can also try to experiment with parameters from the `CountVectorizer` method, for example changing the minimum and maximum frequencies of which words are considered, this might increase performance of the model and reduce running time.\n",
    "\n",
    "As a reminder, you should use the validation set to test different settings of your model, and then pick the best model and run this on the test set. For the evaluation on both the validation and the test set you should calculate precision, recall and F1 on the positive class, and use this to select your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3e27a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1b236cd20c94982cd4f1b7d2c4458f9",
     "grade": false,
     "grade_id": "pss3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement the baseline model in this cell\n",
    "# You will most likely get a warning about the maximum number of iterations, this is fine\n",
    "np.random.seed(1234) # do not change this!\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier # use 42 as the random_state!\n",
    "baseline_model = None\n",
    "baseline_model_predictions=None\n",
    "\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfeec0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7191096541a0216cbf1d735099034fa9",
     "grade": true,
     "grade_id": "pss3a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do you get the right output shape?\n",
    "assert_equal(baseline_model_predictions.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef760b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b4d7aa6ac1e19a9b653238721ae0eb6",
     "grade": false,
     "grade_id": "cell-37d15f3c2af1b490",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the cell below, implement you own MLP network, where you finetune the parameters on the validation set, and report the predictions of you best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44648ae",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c21ebb76d1b0581bc48ae3f0c5a8332b",
     "grade": true,
     "grade_id": "pss3t",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement your own tuned MLP model in this cel\n",
    "my_model = None\n",
    "my_model_predictions = None\n",
    "# Implement the training and evaluation below\n",
    "\n",
    "#WRITE YOUR CODE HERE\n",
    "\n",
    "print(my_model_predictions.shape)\n",
    "assert_equal(my_model_predictions.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a5e80",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec604130fff7456185308ceeef645e88",
     "grade": false,
     "grade_id": "pss4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 4: Evaluation\n",
    "\n",
    "Now that you have implemented and tuned your network it is time to evaluate the network on the held out test set. For this calculate precision, recall and F1 for the positive on the positive class, and report the results for both the baseline and your own model. You can use the `classification_report` function, but explain what you look at, micro, macro, or weighted, and why? As with the previous exercises, make a nice report of the P,R and F1 scores of the single node baseline and you method in a table where you show the differences between the methods, also feel free to make some nice plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084c7de",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "521d62e56bb2d3ebba7eddda38a3d4e5",
     "grade": true,
     "grade_id": "pss4t",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "baseline_P, baseline_R, baseline_F1 = None, None, None\n",
    "my_model_P, my_model_R, my_model_F1 = None, None, None\n",
    "\n",
    "#WRITE YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
