{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e9f87d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "privacy",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment Applied Machine Learning BSc IK \n",
    "\n",
    "## Notebook made by\n",
    "\n",
    "**Gebruik graag dit formaat**\n",
    "\n",
    "* Voor de namen:  voornaam rest van je naam, voornaam rest van je naam,....\n",
    "* je studentnummers: hetzelfde: scheidt met `,`\n",
    "* je emails: hetzelfde: scheidt met `,`\n",
    "* voor je groep: **alleen de hoofdletter** (iets als  `A` of `B` dus)\n",
    "\n",
    "__Namen__:Anoniem",
    "\n",
    "__Emails__:Anoniem",
    "\n",
    "__Student id__:Anoniem",
    "\n",
    "__Groep__:Anoniem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80a53f0",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* Een aantal opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. Dit zijn dus *NIET* alle tests, die komen er bij het graden nog bij.\n",
    "* Elke vraag is 1 punt waard, tenzij anders aangegeven. Soms is die punt onderverdeeld in deelpunten, maar niet altijd. \n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren\n",
    "\n",
    "## Na het inleveren!\n",
    "\n",
    "* Het gebeurt erg vaak dat mensen een \"leeg bestand\" inleveren. Vaak een andere versie van de opgave die nog ergens op je computer rondslingerde. Zonde van al je werk toch!\n",
    "* Dus, lever **minstens een half uur voor tijd in**. Download dan wat je hebt ingeleverd op Canvas. Geef het een andere naam om verwarring te voorkomen. En draai alle cellen, en bekijk het. Geen syntax fouten? Alle vragen gemaakt? Dan zit het vast wel goed, en hoef je niet in de zenuwen te zitten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f8d91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b093f381ee492221a8bc06649c8601fd",
     "grade": false,
     "grade_id": "i",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Applied Machine Learning W3\n",
    "\n",
    "In the assignment for this week the focus will be on evaluation, where we take a look at confusion matrices, different metrics, and evaluation techniques such as cross validation and confidence intervals. As always, the exercises are mostly independent of each other, so if you get stuck somewhere, try doing a different part of the assignment, and come back to it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c901ff1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2a998a1b45fc2c2bfb5ec35a33a7724",
     "grade": false,
     "grade_id": "cell-17d642854ee293ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Index\n",
    "\n",
    "1. [Confusion Matrices](#conf_mat)\n",
    "2. [Evaluation DIY](#eval1)\n",
    "3. [Evluation with sklearn](#eval2)\n",
    "4. [Cross Validation & Confidence Intervals](#cross_val)\n",
    "5. [Baselines](#baselines)\n",
    "6. [More Theory Questions](#theory)\n",
    "7. [contagiousness Vs. Deadliness](#contag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d84b09",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdcc0ffa834e813a679dbabd48d37198",
     "grade": false,
     "grade_id": "imp",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# testing\n",
    "from nose.tools import assert_count_equal, assert_equal, assert_almost_equals\n",
    "from numpy.testing import *\n",
    "from numpy.testing import assert_equal\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "# Please do not remove this: \n",
    "np.random.seed(31415)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec61cd3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd6af72842db55bc91059959b9a997b1",
     "grade": false,
     "grade_id": "cell-d0313968d002d66f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"conf_mat\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ce2ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d98adc42b9c9b213c178b3f2f47fa1bb",
     "grade": false,
     "grade_id": "vw",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "In the class we looked at the following example Confusion Matrix with multiple classes:\n",
    "\n",
    "\n",
    "* example  with 3 classes, N=1000\n",
    "    * priors 700,200,100\n",
    "    * diagonal : 600,50,10\n",
    "    * predicted classes: 900,80,20\n",
    "* Compute, precision, recall, F1 per class and  accuracy, macro values, weighted values over all classes.\n",
    "\n",
    "## Your turn\n",
    "\n",
    "* Create an example yourself, with maybe more classes and completely different numbers.\n",
    "* Compute precision and recall per class alongside the table, indicate very very clearly what you are doing, and the order of things.\n",
    "* Do it on paper with pencil and nice colors, make a photo, put it on the web, and create a hyperlink in the answer cell to your picture(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef38be",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e56e49330e1d114fdac1ad312f90feed",
     "grade": true,
     "grade_id": "vwa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de9129",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6037eb4556e456eb1df4a969b05c861b",
     "grade": false,
     "grade_id": "cell-820a1a4af2bdc91c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"eval1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96879c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ac8cb7975696759736ef6d1cbbb0a55",
     "grade": false,
     "grade_id": "q1q",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Evaluating a model\n",
    "\n",
    "In this exercise we will be creating a complete ML pipeline where we load in a dataset, prepare it for classification by splitting it in train and test, run a classifier, and evaluate the results of this classifier.\n",
    "\n",
    "We start by loading in the dataset, this code is already given for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d001add",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eed72d774f347dd8d089c635bdabfdf0",
     "grade": false,
     "grade_id": "data",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def loadfile():\n",
    "    if 'faults.csv.gz' in os.listdir():\n",
    "        return 'faults.csv.gz'\n",
    "    elif os.path.exists('../../data/Week1/'):\n",
    "        return '../../data/Week3/faults.csv.gz'\n",
    "    elif os.path.exists('../../../data/Week1/'):\n",
    "        return '../../../data/Week3/faults.csv.gz'\n",
    "    \n",
    "df= pd.read_csv(loadfile())\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb935e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9abc2cabdc834380f048c1602638a37d",
     "grade": false,
     "grade_id": "cell-1ef94baba8de2110",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The above dataframe contains data about the faults of machines. It is a multiclass classification problem with 6 fault classes and a 7th \"other\" class. It has 27 explanatory variables. Let's make a piechart of the data so that we can get a bit of a feel for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403afee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79e25c11a167e63d02d1260da483f642",
     "grade": false,
     "grade_id": "target",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "classes= ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains',\n",
    "       'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "df[classes].sum().plot(kind='pie', title='Distribution of the classes among the 1941 instances.');\n",
    "df[classes].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02116c7b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47fc154f0475689bbe2acf2fc1ea6c60",
     "grade": false,
     "grade_id": "cell-8f9ac0272b48d77e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next up is actually running the experiment, which we are going to to in the steps provided below.\n",
    "\n",
    "### Start up\n",
    "1. Add a column `target` to `df` which contains the label names from the `classes` as values and a row has label `X` as value in that column iff `df[X]==1`. (maybe there is a `numpy` function you can use?)\n",
    "2. Drop the original columns of the labels from the dataframe. \n",
    "3. Do a stratified 70-30 data train test split. \n",
    "    * Indicate and check visually that the data is split stratified, so each class also has a 70-30 split.\n",
    "4. Create `y_pred` using KNN. You may want to normalize the data first.\n",
    "\n",
    "### Evaluate\n",
    "\n",
    "1. Create the confusion table based on `y_pred` and `y_test` from scratch, that is not using any `sklearn` functions. \n",
    "    2. Also show it nicely using `sns.heatmap`. \n",
    "2. Compute precision, recall and F1 for each label.\n",
    "3. Compute accuracy.\n",
    "4. Compute Macro, micro and weighted versions of precision, recall and F1.\n",
    "5. Create at least two non-learned rule based baseline predictions, compute evaluation metrics and put these together with the scores for your KNN model into a clear table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32dca98",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94965af8a1994f30b42ad6d5c7210456",
     "grade": false,
     "grade_id": "cell-ae9a9e52704e94cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Starting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350ef54",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b7b603743a4dd9b4de236a7a36bf731",
     "grade": false,
     "grade_id": "q11",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start by adding the new column\n",
    "\n",
    "# Fill in the right code in the line below\n",
    "df['target'] = None\n",
    "for x in range(df.shape[0]):\n",
    "    for y in classes:\n",
    "        if df.at[x,y] == 1:\n",
    "            df.at[x,'target'] = y\n",
    "new_df = df.drop(classes, axis=1)\n",
    "# after adding the column, make matrices of the data for easy use\n",
    "\n",
    "#WRITE YOUR CODE HERE\n",
    "x = [x for x in range(len(classes))]\n",
    "y = [y for y in classes]\n",
    "z = (x,y)\n",
    "print(z)\n",
    "for a in range(len(new_df)):\n",
    "    for b in z[0]:\n",
    "        if new_df['target'][a] == z[1][b]:\n",
    "            new_df['target'][a] = z[0][b]\n",
    "\n",
    "faults_X, faults_y = np.array(new_df.iloc[:, :-1]), np.array(new_df['target'])\n",
    "\n",
    "T=pd.Series(new_df.target).value_counts()\n",
    "T.plot(kind='pie', title='Distribution of the classes among the 1941 instances.');\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91fb65b-68fe-4fda-a6c8-dd5ac7627684",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b6463822633230ab8df31a80a199c16",
     "grade": true,
     "grade_id": "cell-f72efeebd7e10d21",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert_equal(new_df['target'].min(), 0)\n",
    "assert_equal(new_df['target'].max(), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6abe6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06cac6ed1b6c634622b06780418b4f76",
     "grade": false,
     "grade_id": "cell-3e77154f87be49cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have added the target column to the data we can create the train test split of the data, you may use `sklearn` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3f10d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea26f38b8fc1d6038d7062745cd12c9a",
     "grade": false,
     "grade_id": "q12",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start up split\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = None, None, None, None\n",
    "\n",
    "#WRITE YOUR CODE HERE\n",
    "y = new_df['target']\n",
    "y = np.array(y.astype('int'))\n",
    "X = new_df[new_df.loc[:, new_df.columns != 'target'].columns]\n",
    "train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.3,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f65b68-40b3-4d99-a89e-a8c0e6afee70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf5ef466a65b893209375461ee9b08b6",
     "grade": true,
     "grade_id": "cell-be05b35e89687da6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run the following print statements, if the proportions are similar\n",
    "# it means that stratification is working correctly\n",
    "\n",
    "print(\"Distribution of labels in the train set\")\n",
    "print(pd.Series(train_y).value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Distribution of labels in the test set\")\n",
    "print(pd.Series(test_y).value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7bf97",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0de28c49d9d3e963d475daafa4e98573",
     "grade": false,
     "grade_id": "cell-d8e74da0bb9a5c47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next up is running the K Nearest Neighbours algorithm on the data, complete the function below, where you are allowed to use the sklearn function `KNeighborsClassifier`, we do this in a fucntion so that we can experiment with parameters later if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6285ae",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7f06b98fb738ccce85cd48c829552fa",
     "grade": false,
     "grade_id": "q13",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start up knn and compute the predictions\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn_predict(train_features, train_labels, test_features, num_neighbors=5):\n",
    "    y_pred = None\n",
    "    classifier = KNeighborsClassifier(n_neighbors=num_neighbors)\n",
    "    #WRITE YOUR CODE HERE\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    y_pred = classifier.predict(test_features)\n",
    "#     print(y_pred)\n",
    "    return np.array(y_pred)\n",
    "\n",
    "# DONT CHANGE THE LINE BELOW\n",
    "y_pred = knn_predict(train_X, train_y, test_X, num_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f85be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83cc47a1983a74956f55073957e0d935",
     "grade": true,
     "grade_id": "q1333",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcab0465",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9e324f4ea246ea63eb6880f8b937c14",
     "grade": false,
     "grade_id": "cell-c8faffe9abc94681",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Next up is creating the confusion matrix of the gold standard labels and our predictions. You are not allowed to use sklearn functions for this. Also, make sure that your code will work for any input of a gold standard array and a predicted array, regardless of the number of classes, so do not hardcode this!\n",
    "\n",
    "**Hints**\n",
    "We are following the definition of the confusion matrix where a row is the predicted class and the column is an actual class, like the first example in the HC slides with apples and oranges. (and the opposite of the wikipedia definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887665a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0f528bdbfabe987f9f9bc0fd6d1e944",
     "grade": false,
     "grade_id": "q1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# we expect the rows in the matrix to be predicted classes, and the columns to be the numbers for the gold\n",
    "# standard classses, this is different from the sklearn representation!\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def construct_confusion_matrix(gold_standard_labels, predicted_labels):\n",
    "    # we will give you the first part, we create an empty square matrix\n",
    "    # the size of the number of labels in the gold standard, understand why?\n",
    "    number_of_classes = len(np.unique(gold_standard_labels))\n",
    "    confusion_matrix = np.zeros((number_of_classes, number_of_classes))\n",
    "\n",
    "    #WRITE YOUR CODE HERE\n",
    "    for i in range(len(gold_standard_labels)):\n",
    "#         confusion_matrix[gold_standard_labels[i]][predicted_labels[i]] += 1 #Sklearn & Wikipedia manier (actual = rij, \n",
    "                                                                                                       # pred. = kolom)\n",
    "        confusion_matrix[predicted_labels[i]][gold_standard_labels[i]] += 1 #Hoorcollege manier (actual = kolom, pred = rij)\n",
    "\n",
    "    return confusion_matrix\n",
    "con_matrix = construct_confusion_matrix(test_y, y_pred)\n",
    "con_matrix, #confusion_matrix(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd0a92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "906d89269ee14d28ffc96e8b1420d164",
     "grade": true,
     "grade_id": "cell-e21219e88ed9717f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "486df1ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fad600b456c2e9db10d1a40f16399ba5",
     "grade": false,
     "grade_id": "cell-679432b9edcb080a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the cell below make a nice confusion matrix here using `y_pred` and `sns.heatmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba336bbe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea40c65e77307627655d7206fdbf3a29",
     "grade": true,
     "grade_id": "cell-b20a9f8ee2ed06f4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#WRITE YOUR CODE HERE\n",
    "sns.heatmap(construct_confusion_matrix(test_y, y_pred), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace570d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f1626700cf0c02d26af090b184ef473",
     "grade": false,
     "grade_id": "cell-cbdefa9bfeff267f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Calculating Accuracy, Precision Recall and F1 through the confusion matrix\n",
    "\n",
    "Next up is calculating accuracy, precision, recall and F1. For this exercise you are not allowed to use the functions from sklearn, but you have to implement them yourself using the confusion matrix. Your task is to fill in the functions below, where the behaviour is similar to sklearn, where you can give in an 'average' parameter, which calculates the metrics with different weighting schemes.\n",
    "\n",
    "**Hints**\n",
    "- Try to think about precision and recall in terms of rows and columns in the confusion matrix, this should make the calculations relatively straightforward to implement in numpy.\n",
    "- you can use `np.diag` to get the values on the diagonal of the confusion matrix.\n",
    "- For F1 you can use the functions of precision and recall you wrote, but use the `None` option for weighting and calculate the different weights using these arrays, to avoid successive rounding errors in your results. Then use `np.nan_to_num` on the raw F1 array in case we have precision and recall both equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c14028",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe4c486d8ee817b7d003dff9f2de5479",
     "grade": false,
     "grade_id": "q2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# complete the below functions, where you implement the following types for each metric:\n",
    "# None: return the metric for each label separately, thus returning an array of size num_classes\n",
    "# 'macro': return the macro weighted metric\n",
    "# 'micro': returns the micro weighted metric\n",
    "# 'weighted': returns the class weighted metric\n",
    "\n",
    "# As a warmup, calculate accuracy over the entire test set\n",
    "def accuracy(confusion_matrix): \n",
    "    accuracy = 0.0\n",
    "    #WRITE YOUR CODE HERE\n",
    "    accuracy = sum(np.diag(confusion_matrix)) / sum(sum(confusion_matrix))\n",
    "    return np.round(accuracy,2)\n",
    "\n",
    "def precision(confusion_matrix, average): \n",
    "    assert average in [None, 'macro', 'micro', 'weighted']\n",
    "    #WRITE YOUR CODE HERE\n",
    "    if average == None:\n",
    "        row = np.array([sum(x) for x in confusion_matrix])\n",
    "        precision = np.diag(confusion_matrix) / row\n",
    "    if average == 'macro':\n",
    "        row = np.array([sum(x) for x in confusion_matrix])\n",
    "        prec = np.diag(confusion_matrix) / row\n",
    "        precision = sum(prec) / len(confusion_matrix)\n",
    "    if average == 'micro':\n",
    "        precision = sum(np.diag(confusion_matrix)) / sum(sum(confusion_matrix))\n",
    "    if average == 'weighted':\n",
    "        row = np.array([sum(x) for x in confusion_matrix])\n",
    "        prec = np.diag(confusion_matrix) / row\n",
    "        distri = sum(confusion_matrix) / sum(sum(confusion_matrix))\n",
    "        precision = sum(prec * distri)\n",
    "    return np.round(precision,2)\n",
    "\n",
    "def recall(confusion_matrix, average): \n",
    "    assert average in [None, 'macro', 'micro', 'weighted']\n",
    "    #WRITE YOUR CODE HERE\n",
    "    if average == None:\n",
    "        recall = np.diag(confusion_matrix) / sum(confusion_matrix)\n",
    "    if average == 'macro':\n",
    "        rec = np.diag(confusion_matrix) / sum(confusion_matrix)\n",
    "        recall = sum(rec) / len(confusion_matrix)\n",
    "    if average == 'micro':\n",
    "        recall = sum(np.diag(confusion_matrix)) / sum(sum(confusion_matrix))\n",
    "    if average == 'weighted':\n",
    "        rec = sum(np.diag(confusion_matrix)) / sum(sum(confusion_matrix))\n",
    "        distri = sum(confusion_matrix) / sum(sum(confusion_matrix))\n",
    "        recall = sum(rec * distri)\n",
    "    return np.round(recall,2)\n",
    "    \n",
    "def f1(confusion_matrix, average):\n",
    "    assert average in [None, 'macro', 'micro', 'weighted']\n",
    "    F1 = 0.0\n",
    "    #WRITE YOUR CODE HERE\n",
    "    if average == None:\n",
    "        row = np.array([sum(x) for x in confusion_matrix])\n",
    "        F1 = np.diag(confusion_matrix)/(np.diag(confusion_matrix) + 0.5*(row + sum(confusion_matrix) - (np.diag(confusion_matrix)*2)))\n",
    "#         F1 = 2*((recall(confusion_matrix,None)*precision(confusion_matrix,None))/(recall(confusion_matrix,None)+precision(confusion_matrix,None)))\n",
    "    if average == 'macro':\n",
    "        row = np.array([sum(x) for x in confusion_matrix])\n",
    "        f1 = np.diag(confusion_matrix)/(np.diag(confusion_matrix) + 0.5*(row + sum(confusion_matrix) - (np.diag(confusion_matrix)*2)))\n",
    "#         f1 = 2*((recall(confusion_matrix,None)*precision(confusion_matrix,None))/(recall(confusion_matrix,None)+precision(confusion_matrix,None)))\n",
    "        F1 = sum(f1) / len(confusion_matrix)\n",
    "    if average == 'micro':\n",
    "        F1 = sum(np.diag(confusion_matrix)) / sum(sum(confusion_matrix))\n",
    "    if average == 'weighted':\n",
    "        row = np.array([sum(x) for x in confusion_matrix])\n",
    "        f1 = np.diag(confusion_matrix)/(np.diag(confusion_matrix) + 0.5*(row + sum(confusion_matrix) - (np.diag(confusion_matrix)*2)))\n",
    "#         f1 = 2*((recall(confusion_matrix,None)*precision(confusion_matrix,None))/(recall(confusion_matrix,None)+precision(confusion_matrix,None)))\n",
    "        distri = sum(confusion_matrix) / sum(sum(confusion_matrix))\n",
    "        F1 = sum(f1 * distri)\n",
    "    return np.round(F1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cfc3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d33379f6c9dcde7f09d391f9f0734cd9",
     "grade": true,
     "grade_id": "q25353",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_matrix = np.array([[1, 2, 0],\n",
    "                       [0, 3, 1],\n",
    "                       [0, 5, 2]])\n",
    "\n",
    "assert_equal(type(precision(test_matrix, None)), np.ndarray)\n",
    "assert_equal(type(recall(test_matrix, None)), np.ndarray)\n",
    "assert_equal(type(f1(test_matrix, None)), np.ndarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3025f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b65a6c43cf9b948e8033f50ca9123117",
     "grade": true,
     "grade_id": "q4",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute Macro, micro and weighted versions of precision, recall and F1.\n",
    "# TODO: also use round to check here to make sure that we dont get small rounding differences\n",
    "test_matrix = np.array([[1, 2, 0],\n",
    "                       [0, 3, 1],\n",
    "                       [0, 5, 2]])\n",
    "\n",
    "assert_equal(type(precision(test_matrix, \"macro\")), np.float64)\n",
    "assert_equal(type(recall(test_matrix, \"macro\")), np.float64)\n",
    "assert_equal(type(f1(test_matrix, \"macro\")), np.float64)\n",
    "\n",
    "assert_equal(type(precision(test_matrix, \"micro\")), np.float64)\n",
    "assert_equal(type(recall(test_matrix, \"micro\")), np.float64)\n",
    "assert_equal(type(f1(test_matrix, \"micro\")), np.float64)\n",
    "\n",
    "assert_equal(type(precision(test_matrix, \"weighted\")), np.float64)\n",
    "assert_equal(type(recall(test_matrix, \"weighted\")), np.float64)\n",
    "assert_equal(type(f1(test_matrix, \"weighted\")), np.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810bc587",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03a8b1e4e942de002213cbe9fc3fcfb8",
     "grade": false,
     "grade_id": "cell-f7263a05fc5a2d43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"eval2\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b96ba9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d81cc1402fcabc84c6926b37bb57d67",
     "grade": false,
     "grade_id": "ev2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Evaluate again\n",
    "\n",
    "1.  Figure out how you can make a nice evaluation report using `sklearn`. Make such a report with `sklearn`.\n",
    "2. Then make the same report using yoiur own functions. You need to add the support, and possibly more. Do that of course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f9168",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a2e227393b0e5a343db917c69ba1108",
     "grade": false,
     "grade_id": "cell-6c9ce99ce465de88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#tip: nice sklearn summary table that gives precision, recall, f1, support. Use that ;)\n",
    "Also make the same report with your own functions, think about support as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902885fc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f754e68b6854beed981d964308d61bc",
     "grade": true,
     "grade_id": "ev2a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, y_pred))\n",
    "recall(con_matrix, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccce10",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74023b41590c6964a556386f2e9cd770",
     "grade": false,
     "grade_id": "cell-7b0b0aebfdcd2ac0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"cross_val\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e387a3b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5031ca6fbb0b3820b1510bc124e83457",
     "grade": false,
     "grade_id": "cv",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Cross validation\n",
    "\n",
    "* Do 5 fold cV for the KNN Classifier that we used in the previous exercise and the faults dataset, using the original `faults_X` and `faults_y arrays` we defined in the beginning.\n",
    "* Does that make sense for the small classes? Think, experiment, look, and make a decision.\n",
    "* Then run it. \n",
    "* Show for each class the distribution of the F1 values. Choose yourself how.\n",
    "* Compute the confidence intervals. \n",
    "* What can you conclude?\n",
    "\n",
    "For this exercise your are allowed to use functions from sklearn, and also use the `f1_score` from sklearn.\n",
    "\n",
    "**Hints**\n",
    "- If you calculate the F1 for each class individually, you can use a numpy array to store the results for each split, and then use numpy functions to easily calculate the confidence intervals.\n",
    "\n",
    "Remember, we calculate confidence intervals for a class using $\\mu \\pm \\frac{\\sigma}{\\sqrt{k}}$ where k is the number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbca5d1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea49d99c6ff731ac9d569f0184264333",
     "grade": true,
     "grade_id": "cvaa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Don't forget about the `knn_predict` function you wrote!\n",
    "\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74c290",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69f4d9bf56844ed33d81f2b589cbf91b",
     "grade": false,
     "grade_id": "cell-0c4fd92462e86b67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"baselines\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278f799",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "217eef2b7b1fb57793ee707fd95e7825",
     "grade": false,
     "grade_id": "baseline",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Baselines\n",
    "\n",
    "Next we are going to implement some very simple baselines that do not use any information from the features, but instead rely only on the label. As with the previous functions, make sure that they work for any label column, so don't hardcode things! As these functions don't rely on the input features , the only input you get is the y label column, and a parameter that specifies the number of samples in the test set.\n",
    "\n",
    "## Majority class\n",
    "\n",
    "* Always choose the majority class\n",
    "\n",
    "## Random \n",
    "\n",
    "* With $C$ classes, let each class have $\\frac{1}{C}$ chance.\n",
    "\n",
    "## Weighted Random \n",
    "\n",
    "*  Let each class have $P(C)$ chance.\n",
    "    * $P(C)$ = fraction of C-instances in the population.\n",
    "    \n",
    "## Compute \n",
    "\n",
    "* Make a clear table with macro, micro, and weighted versions precision, recall and F1 for these three baselines for the `faults` dataset.  Clearly indicate how and why you decided to calculate things the way you did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29ee92",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19e09fd5a4db47f4c92988a8a2cf876b",
     "grade": false,
     "grade_id": "baselinea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Majority class\n",
    "from scipy.stats import mode # what does this function return, how can you use it?\n",
    "\n",
    "def majority_class(input_labels, test_size):\n",
    "    output_labels = []\n",
    "    #WRITE YOUR CODE HERE\n",
    "    return np.array(output_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37c03c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eee68bcab51b829cd9d1aae15e4f0c6e",
     "grade": true,
     "grade_id": "cell-b81e0fa41487d31c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(majority_class([0, 1, 0, 1], test_size=20)), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59803a59-91b3-4303-ad69-c1269f46c4a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51db3c4ada581d2a9f85d85975b3ccea",
     "grade": false,
     "grade_id": "cell-bc805a237f704eab",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "\n",
    "def random(input_labels, test_size): \n",
    "    output_labels = []\n",
    "    #WRITE YOUR CODE HERE\n",
    "    return np.array(output_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082868be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88e686a6470926183502b79e50141625",
     "grade": true,
     "grade_id": "cell-1a365d9077f59356",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(random([0, 1, 0, 1], test_size=20)), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95a5ad",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06bd90ef27e3329808fe685a9d32a81e",
     "grade": false,
     "grade_id": "cell-9f81af4ac37df57f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hint: have a look at the numpy.random.choice function with a distribution\n",
    "# Another hint: you can get counts of unique values with np.unique (look at the documentation)\n",
    "# which you can use to the probabilities for your distribution\n",
    "from numpy.random import choice\n",
    "\n",
    "def weighted_random(input_labels, test_size):\n",
    "    output_labels = []\n",
    "    #WRITE YOUR CODE HERE\n",
    "    return np.array(output_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a685306",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c9a7617e989106a71dbd7b2a72559b2",
     "grade": true,
     "grade_id": "cell-ba75f549511618ae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(random([0, 1, 0, 1], test_size=20)), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6c899",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d43700aac70208ebf8da7eeba823880",
     "grade": false,
     "grade_id": "cell-ba14eb817d76890b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now construct your table with results using the functions we just wrote above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af52c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "705037543b4a1b0b360644659858b7a1",
     "grade": true,
     "grade_id": "cell-38debba006f4fad0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c542c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67801d95300b43d5abff5d6351ee1fe5",
     "grade": false,
     "grade_id": "cell-5fa897c7dbda8349",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id =\"theory\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e6202",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ed95da71836a01f98992dadd85543f0",
     "grade": false,
     "grade_id": "th",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Part 3: theory\n",
    "\n",
    "The following questions are about precision, recall and accuracy.\n",
    "\n",
    "Suppose you have a disease that occurs in 1 in a 100 people, and you have a test with an accuracy of .95. Meaning that in 95 percent of the case you correctly predict whether someone has the disease or not: The sum of the elements on the diagonal divided by the sum of all elements in the confusion matrix is .95.\n",
    "\n",
    "1. What is de maximum (variable `maxR`) and minimal (`minR`) *recall* for the disease-class with these numbers?\n",
    "2. What is de maximum (variable `maxP`) and minimal (`minP`) *precision* for the disease-class with these numbers?\n",
    "\n",
    "**Hint** Construct a confusion matrix where you try different settings, make sure the numbers add up to the right totals.\n",
    "\n",
    "## OLD DUTCH TEXT\n",
    "\n",
    "Deze vragen gaan over precisie, recall en accuraatheid.\n",
    "\n",
    "Stel je hebt een ziekte die bij 1 op de 100 mensen voorkomt. En je hebt een test met een accuraatheid van .95. Dit laatste betekent dat je in 95% van de gevallen juist voorspelt of iemand de ziekte heeft of niet. Anders gezegd: de som van de elementen op de diagonaal van de confusion matrix gedeeld door het totaal aantal elementen is .95.\n",
    "\n",
    "1. Wat is de maximale(variabele `maxR`) en minimale (`minR`) *recall* voor de ziekte-klasse bij deze gegevens?\n",
    "2. Wat is de maximale (`maxP`) en minimale (`minP`) *precision* voor de ziekte-klasse bij deze gegevens?\n",
    "\n",
    "**Hint** Maak een confusion matrix waarin je verschillende settings uitprobeert. Zorg er steeds voor dat alles netjes optelt. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fee4f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e68ec312daa4a819a80b293ee63e600",
     "grade": false,
     "grade_id": "tha",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "minP = None  # precisie\n",
    "maxP = None\n",
    "minR = None  # recall\n",
    "maxR = None\n",
    "\n",
    "#WRITE YOUR CODE HERE\n",
    "# 1 op de 100 mensen -> 100 op de 10.000 mensen\n",
    "# rijen -> predicted, kolommen -> actual\n",
    "voorbeeld_matrix = np.array([[95, 495],\n",
    "                             [5, 9405]])\n",
    "precision(voorbeeld_matrix, None), recall(voorbeeld_matrix, None) \n",
    "# precision = 0.16 | recall = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670afda",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4147daefc34e8b6378ab82ffcd495794",
     "grade": true,
     "grade_id": "tht",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for m in {minP, maxP, minR, maxR}:\n",
    "    assert 0 <= m <= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a60538",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8386fa56edec98eb4e9cdae4d41336a3",
     "grade": false,
     "grade_id": "th2b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Variant \n",
    "\n",
    "Dezelde vraag als boven, maar nu hebben 5 op de 100 mensen de ziekte en hebben we juist een accuraatheid van .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3017a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b40d9ee7b419bb975e8bf714673bf0b8",
     "grade": false,
     "grade_id": "th2a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "minP = None  # precisie\n",
    "maxP = None\n",
    "minR = None  # recall\n",
    "maxR = None\n",
    "\n",
    "#WRITE YOUR CODE HERE\n",
    "# 5 op de 100 mensen -> 500 op de 10.000 mensen\n",
    "# accuracy = .99\n",
    "# rijen -> predicted, kolommen -> actual\n",
    "voorbeeld_matrix2 = np.array([[495, 95],\n",
    "                              [5, 9405]])\n",
    "precision(voorbeeld_matrix2, None), recall(voorbeeld_matrix2, None)\n",
    "# precision = 0.84 | recall = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1392562",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8cae35434829e05de6cbbe0a82f9861",
     "grade": true,
     "grade_id": "th2t",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for m in {minP, maxP, minR, maxR}:\n",
    "    assert 0 <= m <= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd92de2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "beaab30797b4c2fcea880fc25f10f1bb",
     "grade": false,
     "grade_id": "cell-cdff11c95603116e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"contag\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a62ca1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df8d6edb01f675d52da6941853da2409",
     "grade": false,
     "grade_id": "c1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Part 4: What is worse: contagiousness of or deadliness?\n",
    "\n",
    "\n",
    "Source: <https://www.nrc.nl/nieuws/2020/12/31/verdachte-virusvariant-met-opvallend-veel-mutaties-a4025804>\n",
    "\n",
    "> What is worse: contagiousness or deadliness? At first glance you might think the second is. In the media there is thus also a sense of relief that the new variant of the virus is more more contagious, but less deadly than the existing variants. But is this justified? Adam Kucharski of the London School of Hygiene and Tropical Medicine [argued on Twitter](https://twitter.com/AdamJKucharski/status/1343567425107881986) that it is not.He gave the following numerical example\n",
    "\n",
    ">Suppose R = 1.1. This means that on average ten people will infect eleven other people. Now suppose the generation time is 6 days i.e. it takes 6 days for those ten people to have infected these eleven people. This in 30 days this 'tree of contagion' splits 5 times, once each six days. Now suppose that of all people that become infected 0.8 percent die of the illness and the at the start we started with on day zero with 10.000 infected people. Then after one month we have $10.000 × (1,1)^5 × 0,8\\% = 129$ people that have died.\n",
    "\n",
    ">What happens when a mutation causes the virus to become 50 percent more deadly? If we will in the above formula with the changed deadliness we get the following: (1,1)^5 × (0,8\\% × 1,5) = 193 $ people dying from the virus.\n",
    "\n",
    ">Now what happens when the virus becomes 50 percent more contagious? Again, we can change the contagiousness in the formula and recalculate: $10.000 × (1,1 × 1,5)^5 × 0,8\\% = 978$, meaning we now have almost a thousand people dying from the virus\n",
    "\n",
    ">How is this possible? The deadliness of the virus is linearly correlated with the number of infrected people, while the number of infected people is related exponentially with the number of infected people, which we can also observe in the formula.\n",
    "\n",
    "* Run the code below and try to understand what happens, do you understand why there is a difference in the number of dead in period 0, why it is the same for period 1, and why it starts to diverge after period 1?\n",
    "* Check that the number of dead at the end of the month in the article is the same as the number at week 5 in the table.\n",
    "* And see how we can very easily make a plot from spreadsheet data like this? We will be using this more in the coming weeks and explore even more cool ways of plotting data and results.\n",
    "\n",
    "## Is it actually correct?\n",
    "1. The formula is of course correct, however the outcome is not. Because if you have an R of 1.1 for a month you ahve had 6 'generations', and the total number of infected people is much higher than $10.000 x (1,1)^5$. Calculate how much more in the fuction `aantal_geinfecteerden_na_periodes`.\n",
    "\n",
    "2. The same goes for the number of people that have died after a month, calculate this as well.\n",
    "\n",
    "\n",
    "\n",
    "# OLD DUTCH VERSION\n",
    ">Wat is erger: besmettelijkheid of dodelijkheid? Op het eerste gezicht denk je het tweede. In de media is dan ook opluchting te bespeuren dat het virus weliswaar besmettelijker is, maar geen ernstiger ziekteverloop veroorzaakt. Maar is dat terecht? Adam Kucharski van de London School of Hygiene and Tropical Medicine [betoogde op Twitter](https://twitter.com/AdamJKucharski/status/1343567425107881986) van niet. Hij gaf het volgende getallenvoorbeeld:\n",
    "\n",
    ">Stel, R is 1,1. Dat wil zeggen dat tien besmette personen gemiddeld elf anderen besmetten. Stel, de generatietijd is zes dagen: het duurt zes dagen tot die tien mensen elf anderen hebben besmet, dus in 30 dagen vertakt die ‘besmettingsboom’ zich vijf keer. Stel, van alle besmette mensen overlijdt 0,8 procent. En stel, op tijdstip nul zijn 10.000 mensen besmet. Dan levert dat na een maand $10.000 × (1,1)^5 × 0,8\\% = 129$ doden op.\n",
    "\n",
    ">Wat gebeurt er dan als het virus door een mutatie 50 procent dodelijker wordt? Dan zijn na een maand $10.000 x (1,1)^5 × (0,8\\% × 1,5) = 193 $\n",
    "mensen overleden.\n",
    "\n",
    ">Als het virus door een mutatie echter 50 procent besmettelijker wordt, dan ligt het dodental bijna vijf keer zo hoog: $10.000 × (1,1 × 1,5)^5 × 0,8\\% = 978$ doden.\n",
    "\n",
    "\n",
    ">Hoe dat kan? De dodelijkheid correleert lineair met het aantal doden, maar voor besmettelijkheid is dat verband exponentieel.\n",
    "\n",
    "* Run onderstaande code. Snap wat er gebeurt. Snap je het verschil in aantal doden voor periode 0. En waarom is het gelijk in periode 1, en gaat het daarna (steeds sneller) uit elkaar lopen?\n",
    "* Check dat het aantal doden aan het eind van de maand in het artikel hetzelfde is als in de tabel bij periode 5.\n",
    "* En kijk eens hoe makkelijk je een plotje van zo'n spreadsheet maakt. Ideaal toch? Later in de cursus leren we hier meer over.\n",
    "\n",
    "## Klopt het wel? (1pt)\n",
    "\n",
    "1. De formule klopt natuurlijk, maar de uitkomst niet. Want als je een maand met een R van 1.1 zit heb je 6 \"generaties\", en dus in totaal veel meer besmette mensen gehad dan $10.000 x (1,1)^5$.  **Hoeveel meer dan?** **Bereken dat in de functie `aantal_geinfecteerden_na_periodes`.**\n",
    "2. En dus hebben we na een maand ook een ander aantal overledenen.  Bereken dat eens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677436a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15db10991c82c2f4d3f1164667e1529f",
     "grade": false,
     "grade_id": "c2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# zoals in het artikel\n",
    "def aantaldoden(aantalbesmet=10**4,\n",
    "                overlijdingsratio=.008,\n",
    "                R=1.1,\n",
    "                hoeveelerger=1.5,\n",
    "                aantalperiodes=5):\n",
    "    return {\n",
    "        'dodelijker':\n",
    "        round(aantalbesmet * (R)**aantalperiodes * overlijdingsratio *\n",
    "              hoeveelerger),\n",
    "        'besmettelijker':\n",
    "        round(aantalbesmet * (R * hoeveelerger)**aantalperiodes *\n",
    "              overlijdingsratio)\n",
    "    }\n",
    "\n",
    "\n",
    "# 50 % erger\n",
    "A = {P: aantaldoden(hoeveelerger=1.5, aantalperiodes=P) for P in range(0, 6)}\n",
    "A = pd.DataFrame(A).T\n",
    "A.index.name = 'Periode'\n",
    "A.plot(kind='bar', title='Aantal doden van geinfecteerden in periode x.')\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361d765",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80b84b681b0ed01683cd4f5049b08af3",
     "grade": false,
     "grade_id": "c2a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def aantal_geinfecteerden_na_periodes(\n",
    "    aantalperiodes=6,\n",
    "    aantalbesmet=10**4,\n",
    "    R=1.1,\n",
    "):\n",
    "    '''Geef het totaal aantal geinfecteerde mensen terug na aantalperiodes generaties/periodes.\n",
    "    Rond af op een geheel getal.'''\n",
    "    #WRITE YOUR CODE HERE\n",
    "    besmet = 0.0\n",
    "    besmet += aantalbesmet * (R)**aantalperiodes\n",
    "    return besmet\n",
    "\n",
    "B = pd.Series({\n",
    "    P: round(aantal_geinfecteerden_na_periodes(aantalperiodes=P))\n",
    "    for P in range(21)\n",
    "})\n",
    "B.plot(kind='bar')\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556de592",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "272cbd6ccb2e37ab9b484d9310c3085c",
     "grade": true,
     "grade_id": "c2t",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(aantal_geinfecteerden_na_periodes(), int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad279a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "042d881891d1d0feedc4e0a6ea5e3efe",
     "grade": false,
     "grade_id": "cc1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Altered dataframe (1pt)\n",
    "\n",
    "Example the code above again, where we construct teh dataframe `A`. Alter the code such that you print the correct numbers acorrding to the text above, and store this in dataframe `AA`. `AA` will have the same shape and index as `A`, just different numbers\n",
    "\n",
    "* Also observe the difference between the titles of the two plots.\n",
    "\n",
    "\n",
    "## Aangepast dataframe (1pt)\n",
    "\n",
    "Bekijk de code hierboven nog eens waarin het Dataframe `A` gemaakt werd. Pas die code aan, zodat je daarmee het dataframe `AA` maakt dat de juiste aantallen print. `AA` heeft dezelfde *shape* als `A`, en ook dezelfde index en kolom namen. Alleen de waardes zijn anders. \n",
    "\n",
    "* Apprecieer ook het verschil in de titels tussen de twee plotjes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7fd2e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c72cf90703cc0b139f2e333f45077971",
     "grade": false,
     "grade_id": "cc1a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "AA = A  # Change with your answer\n",
    "\n",
    "#WRITE YOUR CODE HERE\n",
    "AA = A.cumsum()\n",
    "T = 'Totaal aantal doden na x periodes van 6 dagen.'\n",
    "AA.plot(kind='bar', title=T)\n",
    "print(AA)\n",
    "\n",
    "\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e2dd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07711104e8c2da3ce90f817e18977d3c",
     "grade": true,
     "grade_id": "cc1t",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(AA.shape, (6, 2))  # 2 kolommen, 6 rijen\n",
    "assert_equal(list(AA.index) == list(range(6)),\n",
    "             True)  # de index van AA is [0,1,2,3,4,5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48cb701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
