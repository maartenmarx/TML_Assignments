{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65918ee9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert Week5_T1.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ed4bb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  sklearn import linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12e6da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AML Week5\n",
    "\n",
    "## Supervised learning: linear regression\n",
    "\n",
    "\n",
    "### topics\n",
    "\n",
    "*   How to fit the best line?\n",
    "    *   Loss function\n",
    "    *   Gradient descent\n",
    "* Preventing overfitting\n",
    "* Polynomial regression\n",
    "* Gradient descent\n",
    "    *   (partial) derivatives\n",
    "\n",
    "\n",
    "### Stof\n",
    "\n",
    "* Chapter 2 Introduction to Machine Learning with Python\n",
    "    * Until (and including) page 57.\n",
    "* Section 4.5 and 4.6  Introduction to Machine Learning with Python\n",
    "* [Data Science Handbook 5.06 linear regression](https://github.com/jakevdp/PythonDataScienceHandbook/blob/8a34a4f653bdbdc01415a94dc20d4e9b97438965/notebooks/05.06-Linear-Regression.ipynb)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6132b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our problem this week\n",
    "\n",
    "> find the best fitting line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b89204",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "print(tips.shape)\n",
    "sns.regplot(x='total_bill', y='tip', data=tips);\n",
    "tips.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c054ce3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our task\n",
    "\n",
    "### Given\n",
    "\n",
    "* A lot of data instances with explanatory variables, `X`, and to be predicted numerical values `y`. \n",
    "\n",
    "### Find\n",
    "\n",
    "* The \"line\", the linear model, best predicting `y` given `X`.\n",
    "\n",
    "### When `X` has only one dimension (one column)\n",
    "\n",
    "* find best **intercept** $a$ and **slope** $b$ such that $a+bx$ best predicts y.\n",
    "\n",
    "### With more dimensions\n",
    "\n",
    "$$ y= w_0 + w_1x_1 + w_2x_2+ ....w_nx_n$$\n",
    "\n",
    "* find the best *hyperplane*\n",
    "* simply find an intercept and a weight for each variable/feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd095c7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In 2 dimensions\n",
    "\n",
    "![](https://online.stat.psu.edu/onlinecourses/sites/stat508/files/lesson02/image_01.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96610e33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How we measure best?\n",
    "\n",
    "\n",
    "* each $y_i$ is the **true value** for $x_i$\n",
    "* each $h(x) = w_0 + w_1 x_i$ is the **predicted value** for $x_i$.\n",
    "* Find those weights (intercept and slope) that **minimize sum of the squared errors**.\n",
    "\n",
    "$$ \\sum_{i=1}^{i=n}(h(x_i)-y_i)^2$$\n",
    "\n",
    "$$h(x) = w_0 + w_1 x.$$\n",
    "\n",
    "![](https://miro.medium.com/max/1222/1*jopCO2kMEI84s6fiGKdXqg.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37d51c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How we measure best?\n",
    "\n",
    "### Theoretically\n",
    "\n",
    "* Thus find those $w_0$ and $w_1$ such that $\\sum_{i=1}^{i=n}(h(x_i)-y_i)^2$ is **minimal**.\n",
    "\n",
    "### Algorithmically\n",
    "    \n",
    "* A quadratic function $f$ is minimal at the point where the **derivitive** $f'$ equals 0\n",
    "    \n",
    "![](../img/quadratic_function.gif)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66d585",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Find the minimum\n",
    "\n",
    "* $f(x) = 2x^2 + 4x -6$.\n",
    "* Then the **derivative** is $f'(x)= 4x+4$.\n",
    "* Now find at which $x$, $f'(x)=0$. That is the minimal point of a quadratic function.\n",
    "* Thus $4x+4=0$ exactly when $x=-1$. And at that point $y=f(x)= 2(-1)^2+ 4\\cdot -1 -6$. That equals $-8$.\n",
    "\n",
    "![](../img/quadratic_function.gif)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6719e6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other technique: gradient descent\n",
    "\n",
    "* later today\n",
    "\n",
    "### Maar nog even, bereken de hoek en richting, de *gradient* op elk punt.\n",
    "\n",
    "\n",
    "* Voor een lijn: De hoek, vertelt de schuinte, en de richting of het omhoog of naar beneden gaat.\n",
    "* Bereken voor zeg punt $x=-2$\n",
    "    1. Vul -2 in voor $x$ in de afgeleide ($f'(x)=4x+4$)\n",
    "    2. Dus $f'(-2)=4\\cdot -2 +4= -8+4=-4$.\n",
    "    3. Dus de gradient op punt $x=-2$ is -4.\n",
    "        * Dus de lijn gaat naar beneden.\n",
    "        * als je 1 stapje naar rechts gaat, ga je 4 stapjes naar beneden.\n",
    "            * Klopt! Want $f(-2)=-6$ en $f(-1)=-10$! \n",
    "            * En $f(-3)$?\n",
    "\n",
    "### en de intercept van de raaklijn op $(x,y)$?\n",
    "\n",
    "* Die is $y+ y_{min}$, met $y_{min}$ de minimale waarde van $y$  voor $f(x)$.\n",
    "    * Dus die $y$ waar de afgeleide 0 is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38e831",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Best fitting function\n",
    "\n",
    "* We found the best fitting function **to the data**,\n",
    "* but we want a function which best **generalizes**, also to new unseen data.\n",
    "* We should not **overfit** to the train data set.\n",
    "\n",
    "#### When do we overfit?\n",
    "\n",
    "* when we use many (all) features, and give them large weights.\n",
    "    * a large weight can give even a meaningless variable with low value a big influence.\n",
    "    \n",
    "#### How to avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e476f17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to avoid large weights? \n",
    "\n",
    "* Really easy: add the **sum of the weights** to the loss function.\n",
    "* This is called **regularization**\n",
    "\n",
    "### Fine tuning\n",
    "\n",
    "1. How much do we want to let this \"regulizer\" count? => use a hyperparameter $\\lambda$.\n",
    "2. weights can be positive and negative\n",
    "    * and can thus cancel each other out in a sum.\n",
    "    * take *absolute values* : **lasso** or **l1** regularization $ \\lambda \\sum |w_i|$\n",
    "    * take *squared values*: **ridge** or **l2** regularization $\\lambda \\sum w_i^2.$\n",
    "3. Each weight counts equal. No good.\n",
    "    * Solve by **standardization**\n",
    "        * Z-transform each feature:\n",
    "        * each feature has 0 mean and unit std."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c188a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression if the data is not linear\n",
    "\n",
    "![](../img/polynomial-regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1b0a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adding Data transformations\n",
    "\n",
    "* Section 4.5 and 4.6 of our book.\n",
    "* Use: `PolynomialFeatures`\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# include polynomials up to x ** 3:\n",
    "# the default \"include_bias=True\" adds a feature that's constantly 1 \n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "poly.fit(X)\n",
    "X_poly = poly.transform(X)\n",
    "```\n",
    "\n",
    ">Generate a new feature matrix consisting of **all polynomial combinations**\n",
    "of the features with degree less than or equal to the specified degree.\n",
    "\n",
    "* if an input sample is two dimensional and of the form\n",
    "`[a, b]`, \n",
    "* the degree-2 polynomial features are `[1, a, b, a^2, ab, b^2]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350777f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression if the data is not normally distributed\n",
    "\n",
    "\n",
    "* Regession works best when each variable is approximately **normal distributed**.\n",
    "* Many \"count\" variables are not\n",
    "* but they often can be transformed into a normal distribution\n",
    "* Eg, a variable X is  **log-normal distributed** if log X is normal distributed\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Log-normal-pdfs.png/600px-Log-normal-pdfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ce80d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "* Sec 4.6 in the book\n",
    "* In fact, taking the logarithm, or another uniform transformation is just a preprocessing step, \n",
    "* not really differenty from Z-transformation.\n",
    "\n",
    "### Example from book 4.6, with lognormal X\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) \n",
    "score = Ridge().fit(X_train, y_train).score(X_test, y_test)\n",
    "score\n",
    "  0.622\n",
    "```\n",
    "\n",
    "#### Now take a log transform\n",
    "\n",
    "```\n",
    "X_train_log = np.log(X_train + 1)\n",
    "X_test_log = np.log(X_test + 1)\n",
    "\n",
    "score = Ridge().fit(X_train_log, y_train).score(X_test_log, y_test) \n",
    "score\n",
    "  0.875\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d95632",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit learn drill again\n",
    "\n",
    "1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn.\n",
    "2. **Choose model hyperparameters by instantiating this class with desired values.**\n",
    "4. Fit the model to your data by calling the ``fit()`` method of the model instance.\n",
    "   - Hier leer je de _vrije parameters_ van het model uit de (trainings) voorbeelden.\n",
    "5. Apply the Model to new data:\n",
    "   - For supervised learning, often we predict labels for unknown data using the ``predict()`` method.\n",
    "   - For unsupervised learning, we often transform or infer properties of the data using the ``transform()`` or ``predict()`` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a59d49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Zoom in on step 2: Grid search \n",
    "\n",
    "* Finding the best **hyperparameter settings** for your data and model  combination.\n",
    "* You do that using a **validation set**\n",
    "    * Split data into three non overlapping parts: train, validation, test.\n",
    "    * Only at the final evaluation step you use test (in best case just once)\n",
    "    \n",
    "### Many choices: really a grid of them\n",
    "\n",
    "* how much regularization? ($\\lambda$)\n",
    "* what kind? Lasso or Ridge\n",
    "* how many extra polynomials and interactions\n",
    "* which variables to transform and how\n",
    "\n",
    "##### With scikit learn's grid search you try them all out  \n",
    "\n",
    "* train with those settings\n",
    "* test on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f99a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Alternative to validation set : Cross validation \n",
    "\n",
    "\n",
    "**Of course, on the original train set.**\n",
    "\n",
    "Handy when you are low on labelled data.\n",
    "\n",
    "![](../img/Cross-Validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c625b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "* One of the scariest topics in ML, because we have to ..... calculate!\n",
    "    * taking derivatives\n",
    "    * algebra\n",
    "\n",
    "### What is it?\n",
    "\n",
    "* A clever way of quickly finding the best regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02612605",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Best regression line?\n",
    "\n",
    "**What was that again?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c7808",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "sns.regplot(x='total_bill', y='tip', data=tips);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64850a57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Best regression line given the data\n",
    "\n",
    "* The line that **minimizes** the mean squared error.\n",
    "\n",
    "$$ \\frac{\\sum (\\hat{y_i}- y_i)^2}{n}$$\n",
    "\n",
    "* where $\\hat{y_i} = \\theta_0 + \\theta_1 x_i$, is the **predicted value** for $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542d529",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Thus find those $\\theta_0$ and $\\theta_1$  which give the smallest\n",
    "\n",
    "$$ \\frac{\\sum ((\\theta_0 + \\theta_1 x_i)- y_i)^2}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d21c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our goal\n",
    "\n",
    "*  find those $\\theta_0$ and $\\theta_1$  which give the smallest\n",
    "\n",
    "$$ \\frac{\\sum ((\\theta_0 + \\theta_1 x_i)- y_i)^2}{n}$$\n",
    "\n",
    "###  simplify and reformulate\n",
    "\n",
    "* vergeet die $\\theta_0$ even\n",
    "* eigenlijk is dan de $\\theta_1$ de variabele, de rest (die $x_i$ en $y_i$) zijn allemaal constanten\n",
    "* dus eigenlijk een functie van de vorm $y=ax^2 +bx +c$\n",
    "* Hoe ziet die er ook alweer uit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8091e65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "![](../img/gradientdescent.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70018268",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient \n",
    "\n",
    "* Wordt gegeven door de derivative, de **afgeleide**\n",
    "* De afgeleide geeft de **hoek** en de **richting** van de curve op elk punt.\n",
    "* De afgeleide van $4x^2 + 4x + 1$ is ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7ef29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* De afgeleide van $ax^2 +bx+c$ is $2ax+b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b126c31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent in action\n",
    "\n",
    "* We hebben een parabool gegeven door $f(x)= 4x^2 + 4x + 1$.\n",
    "* De afgeleide is dus $f'(x)=8x+4$.\n",
    "* Nu gaan we met gradient descent proberen in een aantal stapjes het minimun te vinden.\n",
    "\n",
    "### Learning rate $\\alpha$\n",
    "\n",
    "* hoe grote stapjes neem je: *default* vaak  $0.01$.\n",
    "\n",
    "### Gradient descent in stappen\n",
    "\n",
    "* begin met een willekeurige $x$: hypothese: $f(x)$ is minimum.\n",
    "* bereken $f(x)$, en bereken de waarde van de gradient op $x$, dus $f'(x)$. \n",
    "* nieuwe $x$ wordt dan oude $x- alpha \\cdot f'(x)$\n",
    "* ga door tot $x$ niet meer omlaag gaat (dan zit je dus op het minimum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e27e54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def step(x,alpha=.05): \n",
    "    '''Voor f(x)=4x**2 + 4x +1. '''\n",
    "    gradient= 8*x +4\n",
    "    return x - alpha * gradient\n",
    "\n",
    "x=10\n",
    "print(\"old x,gradient,new x\")\n",
    "for _ in range(20):\n",
    "    print(round(x,2),round(8*x+4,1),round(step(x),2))\n",
    "    x=step(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b0980c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# en inderdaad, het minimum ligt op .5\n",
    "\n",
    "![](../img/gradientdescent2.png)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db27bb90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Eeh, \n",
    "\n",
    "* Dit hadden we toch ook gewoon direct kunnen uitrekenen?\n",
    "* $8x+4=0 \\Rightarrow 8x=-4 \\Rightarrow x= - \\frac{4}{8}=-.5$.\n",
    "* Ja, hier wel, maar voor veel functies is dat helemaal niet zo direct of eenduidig.\n",
    "* Dan is gradient descent een heel handig middel om dat minimum te vinden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dfc3bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Afronding en terug naar onze regressie\n",
    "\n",
    "### doel\n",
    "\n",
    "* Vind die $\\theta$ zodat $$\\frac{\\sum( \\theta \\cdot x_i - y_i)^2}{n}$$ zo klein mogelijk is.\n",
    "* we noemen dit de **loss functie** van theta. (ook wel **cost function**)\n",
    "* de afgeleide is  (waarbij $e_i = \\theta x_i - y_i$ de error, de fout,  gemaakt voor instantie $x_i$),\n",
    "$$\\frac{2\\cdot\\sum_{i=1}^{i=n} x_i\\cdot e_i}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5342da8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Even wat algebra en calculus\n",
    "\n",
    "* $(\\theta x_i -y_i)^2$ =\n",
    "* $(\\theta x_i)^2 - 2\\theta x_i y_i + y_i^2$ =\n",
    "* $\\theta^2 x_i^2 - 2\\theta x_i y_i + y_i^2$ \n",
    "\n",
    "Daar de afgeleide mbt $\\theta$ van nemen wordt:\n",
    "\n",
    "* $2x_i^2\\theta - 2x_iy_i$ =\n",
    "* $2x_i(\\theta x_i - y_i)$ =  (want we noemen $\\theta x_i - y_i$, gewoon de fout op $i$, dus $e_i$.\n",
    "* $ 2 x_i \\cdot e_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a8203",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Met meer variabelen\n",
    "\n",
    "* Met $n$ variabelen /features, hebben we deze regressie vergelijking\n",
    "\n",
    "$$ \\theta_0 + \\theta_1x_1 + \\ldots  \\theta_nx_n = y.$$\n",
    "\n",
    "* We moeten nu dus de optimale waardes vinden voor elke $\\theta_i$.\n",
    "* Dat doen we door **partiële afgeleides** te nemen, voor elke $\\theta_i$.\n",
    "    * je neemt dan de andere $\\theta_j$ als constanten en \n",
    "    * dan  gaat het net zo als met 1 theta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18177d5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Voorbeeld: vind het minimum\n",
    "\n",
    "![](../img/grad_desc/gd1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683107f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Product regel voor afgeleiden\n",
    "\n",
    "$$ (f(x)\\cdot g(x))'= f'(x)\\cdot g(x) + f(x)\\cdot g'(x)$$\n",
    "\n",
    "* $h(x) = 85- \\frac{1}{90}x^2(x-6)y^2(y-6)$ \n",
    "* $h(x) =  c_1 +c_2 \\cdot f(x)g(x)\\cdot c_3$, with $f(x)=x^2$ and $g(x)=x-6$.\n",
    "* Thus $h'(x)= c_2\\cdot (f'(x)\\cdot g(x) + f(x)\\cdot g'(x))\\cdot c_3$.\n",
    "* Now $f'(x)=2x$ and $g'(x)=1$, thus $f'(x)\\cdot g(x) + f(x)\\cdot g'(x)$ =\n",
    "* Now $2x\\cdot (x-6) + x^2\\cdot 1$ =\n",
    "    * $2x^2- 12x + x^2$ =\n",
    "    * $3x^2-12x$ =\n",
    "    * $x\\cdot(3x-12)$.\n",
    "\n",
    "* Dus de partiele afgeleide tov $x$ = $c_2\\cdot x\\cdot(3x-12) \\cdot c_3$ =\n",
    "\n",
    "$$\\frac{1}{90}x(3x-12)y^2(y-6).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7667a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Opgave\n",
    "\n",
    "1. Vind het minimum analytisch, door de afgeleides op 0 te zetten, en te berekenen voor welke waardes van x en y je 0 vindt.\n",
    "    * Let op, je vindt meer waardes dan alleen het minimum. Je vindt alle plekken in de grafiek waarbij de afgeleide een horizontaal vlak is. \n",
    "    * Maar de meeste zijn \"gek\", en kan je zo wegredeneren.\n",
    "2. Vind het met gradient descent, gebruikmakend van ons eerdere algoritme, maar nu met de 2 partiele afgeleides (gegeven op het plaatje).\n",
    "    * Begin in  x=0.5, y=0.6, met $\\alpha=0.05$. \n",
    "    * Hoe snel loop je naar beneden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c062c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural networks\n",
    "\n",
    "* Over 2 weken\n",
    "* We zullen zien dat het trainen van een neuraal netwerk eigenlijk net zo gaat.\n",
    "* Alleen heb je dan gigantiasch veel parameters,\n",
    "* en meestal een wat ingewikkelder loss/cost functie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48abecf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wat hebben we gedaan? Lineare Regressie.\n",
    "\n",
    "1. Wat is het? Wat is het resultaat?\n",
    "2. Wat is het optimalisatiecriterium? De loss/cost functie.\n",
    "3. Hoe vinden we nou die beste lijn/hyperplane?\n",
    "    * gradient descent\n",
    "    \n",
    "### Zij-onderwerpen\n",
    "\n",
    "* Regularisatie: lasso en ridge\n",
    "* hyperparameter tuning: grid search en cross validation\n",
    "* Wat te doen als de data nou eenmaal \"niet recht is\"\n",
    "    * Polynomials nemen van de input\n",
    "    * combinaties van input feautues als niuew feature\n",
    "    * feature naar normaal verdeling transformeren.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
