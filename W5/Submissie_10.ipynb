{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0961e3da",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "privacy",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment Applied Machine Learning BSc IK \n",
    "\n",
    "## Notebook made by\n",
    "\n",
    "**Gebruik graag dit formaat**\n",
    "\n",
    "* Voor de namen:  voornaam rest van je naam, voornaam rest van je naam,....\n",
    "* je studentnummers: hetzelfde: scheidt met `,`\n",
    "* je emails: hetzelfde: scheidt met `,`\n",
    "* voor je groep: **alleen de hoofdletter** (iets als  `A` of `B` dus)\n",
    "\n",
    "__Namen__:Anoniem",
    "\n",
    "__Emails__:Anoniem",
    "\n",
    "__Student id__:Anoniem",
    "\n",
    "__Groep__:Anoniem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03cdf6",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* Een aantal opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. Dit zijn dus *NIET* alle tests, die komen er bij het graden nog bij.\n",
    "* Elke vraag is 1 punt waard, tenzij anders aangegeven. Soms is die punt onderverdeeld in deelpunten, maar niet altijd. \n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren\n",
    "\n",
    "## Na het inleveren!\n",
    "\n",
    "* Het gebeurt erg vaak dat mensen een \"leeg bestand\" inleveren. Vaak een andere versie van de opgave die nog ergens op je computer rondslingerde. Zonde van al je werk toch!\n",
    "* Dus, lever **minstens een half uur voor tijd in**. Download dan wat je hebt ingeleverd op Canvas. Geef het een andere naam om verwarring te voorkomen. En draai alle cellen, en bekijk het. Geen syntax fouten? Alle vragen gemaakt? Dan zit het vast wel goed, en hoef je niet in de zenuwen te zitten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9576e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d84879f318f2ac86f1d017e1260cc980",
     "grade": false,
     "grade_id": "i",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Applied Machine Learning W5\n",
    "\n",
    "In this week's assignment we will be zooming in on one of the cornerstones of Machine Learning: Gradient Descent. We have set up a few exercises that should get you familiar with how gradient descent works and how different loss functions effect the outcome of the algorithm. The rest of the exercises will be focussed on getting more familiar with linear regression. The asssignment consists of 4 subquestions, for a total number of 13 points.\n",
    "\n",
    "\n",
    "## Subquestions\n",
    "- 1.1 [Gradient Descent](#grad1) <br>\n",
    "- 1.2 [Gradient Descent on the Tips dataset](#grad2) <br>\n",
    "- 2. [Linear Regression](#linreg1) <br>\n",
    "   - 2.1.[The Titanic Dataset and Scaling](#linreg_intro)\n",
    "   - 2.3 [Cross Validation](#crossval) <br>\n",
    "   - 2.4 [Grid Search](#gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12dd95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cf9a5a63dbaff4ef4e2e4eb70500f25",
     "grade": false,
     "grade_id": "imp",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import Tuple\n",
    "from scipy.stats import mode\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# testing\n",
    "from numpy.testing import *\n",
    "from pandas.testing import assert_frame_equal\n",
    "from nose.tools import assert_count_equal, assert_equal, assert_almost_equals\n",
    "\n",
    "# Please do not remove this: \n",
    "np.random.seed(31415)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5490a821",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceb3f8c3a85c17da8fcaf0852a226f77",
     "grade": false,
     "grade_id": "cell-fb09909f7aae7a3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"grad1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9aa95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eef39abb12602a9049d91fb590f3b4af",
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Q1  Gradient descent\n",
    "\n",
    "Do the exercise in the one but last slide of this week, related to the function $f(x,y)$ given there.\n",
    "\n",
    "![](../../data/Week5/gd1.png)\n",
    "1. Find the minimum analytically, by putting the two partial derivatives to 0 and solve for $x$ and $y$. You find a lot of possible value pairs, but most are spurious. Explain why they are spurious, and give the real answer(s).\n",
    "2. Find the minimum using the gradient descent algorithm, using these two partial derivatives. In the slides you find the update step. Program it.  Add a while loop in whichn you check that either you have reached your maximum number of allowed steps (a hyperparameter of course), or $x$ and $y$ make the function go up after following the gradient (i.e., you are in a minumum). \n",
    "3. Experiment with the learning rate, and tabulate the learning rate against the number of steps, and explain what is going on. Also make the learning rate really large so you jump over the minimum. Not only give a nice table (or graphic, also explain what you have learned from about the size of the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64393ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "607628a248428a06d23ec2b444aa2ff2",
     "grade": false,
     "grade_id": "cell-760c3a8ae92bdf3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First find the minimum analytically, and try to explain how you did this, and how you removed the spurious answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f047073",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46907fa96022fab8362b60c39ba58792",
     "grade": true,
     "grade_id": "cell-d6f6348e3a39807c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### dx\n",
    "\n",
    "1. ~~x = 0~~\n",
    "2. ~~y = 0~~\n",
    "3. x = 4\n",
    "4. ~~y = 6~~\n",
    "\n",
    "1 & 2 will always result in 0 for the derivative, even when constants are completely different, and can therefore be ignored (spurious answers).\n",
    "\n",
    "For answer 3:\n",
    "- 3x - 12 = 0\n",
    "- 3x = 12\n",
    "- x = 4\n",
    "\n",
    "For answer 4:\n",
    "- y - 6 = 0\n",
    "- y = 6\n",
    "\n",
    "Answer 4 can be ignored; this is de df / dx formula.\n",
    "\n",
    "### dy\n",
    "\n",
    "1. ~~x = 0~~\n",
    "2. ~~y = 0~~\n",
    "3. ~~x = 6~~\n",
    "4. y = 4\n",
    "\n",
    "1 & 2 will always result in 0 for the derivative, even when constants are completely different, and can therefore be ignored (spurious answers).\n",
    "\n",
    "For answer 3:\n",
    "- x - 6 = 0\n",
    "- x = 6\n",
    "\n",
    "Answer 3 can be ignored; this is de df / dy formula.\n",
    "\n",
    "For answer 4:\n",
    "- 3y - 12 = 0\n",
    "- 3y = 12\n",
    "- y = 4\n",
    "\n",
    "### Answers\n",
    "\n",
    "- x = 4\n",
    "- y = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6040b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ba81e7c7184e5d092e32eefd688a245",
     "grade": false,
     "grade_id": "cell-0674ecd1bffc26f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will start by implementing the function `f` itself, and two two functions that take the partial derivatives with respect to x and y, in `df_dx` and `df_dy` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5976e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "202948b79150d09b0b8bf6c432aa04ec",
     "grade": false,
     "grade_id": "q1a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First, let us define our function and its partial derivatives\n",
    "# Fill in the functions below\n",
    "\n",
    "def f(x,y) -> float:\n",
    "    '''\n",
    "    Function that implements the function f from above, returning a float\n",
    "    '''\n",
    "    return 85 - 1 / 90 * x ** 2 * (x - 6) * y ** 2 * (y - 6)\n",
    "\n",
    "def df_dx(x, y) -> float:\n",
    "    '''\n",
    "    Function that returns the partial derivative of f with respect to x, returning a float\n",
    "    '''\n",
    "    return -1 / 90 * x * (3 * x - 12) * y ** 2 * (y - 6)\n",
    "\n",
    "def df_dy(x, y) -> float:\n",
    "    '''\n",
    "    Function that returns the partial derivative of f with respect to y, returning a float\n",
    "    '''\n",
    "    return -1 / 90 * x ** 2 * (x - 6) * y * (3 * y - 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cea253",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a93bc0379165a3f9e60e5e17f9c0cb04",
     "grade": true,
     "grade_id": "cell-df8abd5e8fa669e8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(f(1.5, 3.0)), float)\n",
    "assert_equal(type(df_dx(1.5, 3.0)), float)\n",
    "assert_equal(type(df_dy(1.5, 3.0)), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77441e9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39cfbf7b6a9f13a3b24ac09a3f25fa05",
     "grade": false,
     "grade_id": "cell-e334b1b68d765bd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After we have implemented these partial derivatives we will now implement the gradient descent algorithm that will, given a learning rate and a maximum number of steps compute the optimal values for x and y. Fill in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0fadd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d862b70d0f773ed8afd3d8fd382b4f52",
     "grade": false,
     "grade_id": "cell-262bf54e21c7bd9b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# To help you along, first implement one step of the gradient descent algorithm in the function `gradient_step` below\n",
    "# using your df_dx and df_dy functions. The function should return a tuple with the new values for x and y\n",
    "def gradient_step(x, y, learning_rate) -> Tuple[float, float]:\n",
    "    x = x - learning_rate * df_dx(x, y)\n",
    "    y = y - learning_rate * df_dy(x, y)\n",
    "    return (x, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f973227",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f06ea52881168a766990762e02ce9785",
     "grade": true,
     "grade_id": "cell-1510b563f85c343b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(gradient_step(0.6, 0.5, 0.01)), tuple)\n",
    "assert_equal(type(gradient_step(0.6, 0.5, 0.01)[0]), float)\n",
    "assert_equal(type(gradient_step(0.6, 0.5, 0.01)[1]), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b6133e",
   "metadata": {},
   "source": [
    "Finally, implement the complete `gradient_descent` algorithm, where you use the function `gradient_step` that you have defined previously. As starting values for x and y, use 0.6 and 0.5 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62947d2e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c0c223dabbaf1163a81a131223ad16f",
     "grade": false,
     "grade_id": "cell-f0b90fc2be2b1d5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate, max_num_steps)->Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    This function implements the gradient descent algorithm for the function f\n",
    "    defined above, given a float learning_rate and an integer max_num_steps.\n",
    "    You can use the functions f, df_dx and df_dy that you created above.\n",
    "    The function should return a tuple containing the optimal x value and the optimal\n",
    "    y value.\n",
    "    \"\"\"\n",
    "    steps = 0\n",
    "    # we will give you some starting values for x and y\n",
    "    x, y = 0.6, 0.5\n",
    "    \n",
    "    for _ in range(max_num_steps):\n",
    "        x, y = gradient_step(x, y, learning_rate)\n",
    "        \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf4fb0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c57271f8206b3d27ec62b1e4b0564bc2",
     "grade": true,
     "grade_id": "cell-ed8571e5e58439a8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(gradient_descent(0.01, 5)), tuple)\n",
    "assert_equal(type(gradient_descent(0.01, 5)[0]), float)\n",
    "assert_equal(type(gradient_descent(0.01, 5)[1]), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9266ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9b0fbb6105a6aa722f0cf937b29f9e8",
     "grade": false,
     "grade_id": "cell-267328d287e8a889",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the cell below, make a nice table/ plot where you experiment with different learning rates and explain what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35886906",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e1650e57c174b85482e1218836884e1",
     "grade": true,
     "grade_id": "cell-c3e605e89721f200",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "max_num_steps = [5, 25, 50, 100, 500]\n",
    "\n",
    "table = pd.DataFrame(index=learning_rates, columns=max_num_steps)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for mns in max_num_steps:\n",
    "        table.loc[lr, mns] = np.round(gradient_descent(lr, mns), 2)\n",
    "\n",
    "display(table)\n",
    "\n",
    "\"\"\"\n",
    "Learning steps 0.01 through 0.20 in the chosen list all arrive at (4.0, 4.0) for x, y.\n",
    "A learning step of 0.50 is too big, overshoots and ends up unable to get to the correct answer.\n",
    "The smaller learning steps need significantly more steps to get to the correct answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6c71e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b7c1b2e0b034afbb5506ecacd873a9e",
     "grade": false,
     "grade_id": "cell-76e2391e627b0469",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"grad2\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53654e33",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "502c9efbea21d8e526d596c5a2482618",
     "grade": false,
     "grade_id": "gd2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Q1b gradient descent on the Tips dataset\n",
    "\n",
    "Now that we have implemented the gradient descent algorithm, we are going to actually use it in a real world dataset, using the `tips` dataset that we have seen before.\n",
    "\n",
    "Run the code below, in which you plot the tip against the total bill. You can see the regression line, and you can also compute the best fitting coefficients using sklearn.\n",
    "\n",
    "But now we ask you to find them yourself using gradient descent. So find the best intercept and slope.\n",
    "\n",
    "### Plot\n",
    "\n",
    "Plot on top of this scatterplot your found line. \n",
    "\n",
    "```\n",
    "tips = sns.load_dataset('tips')\n",
    "sns.regplot(x='total_bill', y='tip', data=tips);\n",
    "```\n",
    "\n",
    "To find the best intercept and slope using gradient descent, we need to define a cost function that measures how well our line fits the data. In this case, we can use the mean squared error (MSE) as our cost function:\n",
    "\n",
    "cost(m, b) = (1/N) * sum((y_i - (m*x_i + b))^2)\n",
    "\n",
    "or, as a nice Latex formula: $\\frac{\\sum\\limits_{i}(y_{i} - (m*x_i + b))^{2}}{N}$\n",
    "\n",
    "where:\n",
    "\n",
    "m: slope b: intercept x_i: total_bill of the i-th observation y_i: tip of the i-th observation N: total number of observations\n",
    "\n",
    "Of course, after you have done this yourself you can check with scikit learn that you got the right values for the parameters :)\n",
    "\n",
    "HINT: in the previous exercise we used `x` and `y` as variables, but in this exercise these variables are `m` and `b`, so be careful that you don't mix up these definitions. As with the previous exercise, implement a maximum number of steps, and also make sure that the new values for m and b do not make the cost function inrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84736df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "tips['total_bill'] = (tips['total_bill'] - tips['total_bill'].mean()) / tips['total_bill'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc705cc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa4d84d78830e2adafc32f0168898197",
     "grade": false,
     "grade_id": "cell-376c0f2db0a8b478",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We load in the dataset\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "def cost_function(m, b, x, y):\n",
    "    cost_sum = [y_i - (m * x_i + b) ** 2 for x_i, y_i in zip(x, y)]\n",
    "    return sum(cost_sum) / len(x)\n",
    "\n",
    "\n",
    "# Define partial derivatives of cost function\n",
    "def partial_m(m, b, x, y):\n",
    "    dm_sum = [-2 * x_i * (y_i - (m * x_i + b)) for x_i, y_i in zip(x, y)]\n",
    "    return sum(dm_sum) / len(x)\n",
    "\n",
    "def partial_b(m, b, x, y):\n",
    "    db_sum = [-2 * (y_i - (m * x_i + b)) for x_i, y_i in zip(x, y)]\n",
    "    return sum(db_sum) / len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32fc510",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d16db300205c05337baea69219132519",
     "grade": true,
     "grade_id": "cell-7ea81339db51ec7a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_test = tips['total_bill']\n",
    "y_test = tips['tip']\n",
    "m_test = 1\n",
    "b_test = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8727a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af4ded7e00fe09604dc0ab9a21c85a2a",
     "grade": true,
     "grade_id": "cell-848d54c273ca6e3d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit line using gradient descent\n",
    "# Fill in the function below, which is similar to the function before, except that we are now working with different\n",
    "# functions for the cost function and partial derivatives.\n",
    "\n",
    "def gradient_descent_tips(x, y, learning_rate, max_num_steps):\n",
    "    \n",
    "    # Initialize slope and intercept\n",
    "    m, b = 0.1, 0.6\n",
    "    \n",
    "    for _ in range(max_num_steps):\n",
    "        new = m - learning_rate * partial_m(m, b, x, y), b - learning_rate * partial_b(m, b, x, y)\n",
    "        m, b = new\n",
    "\n",
    "    # Return best estimates of m and b\n",
    "    return m, b\n",
    "    \n",
    "m, b = gradient_descent_tips(tips['total_bill'], tips['tip'], 0.001, 1000)\n",
    "\n",
    "# Plot scatterplot with regression line and gradient descent line\n",
    "sns.regplot(x='total_bill', y='tip', data=tips)\n",
    "plt.plot(tips['total_bill'], m*tips['total_bill']+b, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28051ce6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbe6d6e3ed6116949db689845d21746f",
     "grade": true,
     "grade_id": "cell-efb7a49d2f199845",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "m, b = gradient_descent_tips(tips['total_bill'], tips['tip'], 0.001, 1000)\n",
    "assert np.allclose(np.shape(m), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e67595",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e92c25a8dec3c81a8db72995b2c9ece4",
     "grade": false,
     "grade_id": "cell-8eb120314fa059c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"linreg1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c7ae0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71312dd00f391dbfb8e82b9b3bcb8e12",
     "grade": false,
     "grade_id": "q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Q2 Cost function for linear regression\n",
    "\n",
    "* Recall the loss/cost function given for linear regression.\n",
    "1.  Often you see it written as not the mean squared error, but half of the mean squared error. Explain in two-three sentences the difference and what the influence of taking the half is.\n",
    "2. Again look at the `tips` dataset with one explanatory variable `total_bill`. Investigate the influence of the few \"high bill\" datapoints on the regression line. Think about the meaning of taking the square of the error. \n",
    "    * Your investigation will take the form a table, with for each of the thresholds we have defined the corresponding optimal coefficient `a` and `b`. It is also nice to plot the lines with these coefficients over the original data, to see the differences.\n",
    "    * Also try to use what we learned on confidence intervals to estimate something on the significance on the observed differences in the regression lines.\n",
    "\n",
    "HINT: You are allowed to use the `LinearRegression` function from sklearn for this exercise. Also, don't round the coefficients, just leave them like they come out of the `LinearRegressor` function. Check the documentation for the function to see how you can get the coefficient and intercept parameters that you want out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88d98a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4b48f71f50691fb25bc1dbb2eaf3630",
     "grade": true,
     "grade_id": "q2at",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd406b6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7907d0b4c6f876acad300ba128d42d39",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "sns.regplot(x='total_bill', y='tip', data=tips,  label='full fit');\n",
    "\n",
    "# It is convenient to iterate over a list of thresholds that truncate the data\n",
    "# You should perform Linear Regression on each of the truncated datasets and put your results in the \n",
    "# variable `threshold_table`, with the column names 'coefficient' and 'intercept'.\n",
    "\n",
    "thresholds = [25, 30, 35, 40, 45]\n",
    "threshold_table = pd.DataFrame()\n",
    "threshold_table.index = thresholds\n",
    "threshold_table['coefficient'] = 0.0\n",
    "threshold_table['intercept'] = 0.0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    threshold_tips = tips[tips['total_bill'] > threshold]\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    model.fit(threshold_tips[['total_bill']], threshold_tips['tip'])\n",
    "    \n",
    "    threshold_table.loc[threshold, 'coefficient'] = model.coef_[0]\n",
    "    threshold_table.loc[threshold, 'intercept'] = model.intercept_\n",
    "\n",
    "# If we want to plot this, we should define a range of x values to plot this for.\n",
    "x = np.arange(0, 50)\n",
    "\n",
    "sns.scatterplot(x=\"total_bill\", y=\"tip\", data=tips)\n",
    "for threshold in thresholds:\n",
    "    coefficient = threshold_table.loc[threshold, 'coefficient']\n",
    "    intercept = threshold_table.loc[threshold, 'intercept']\n",
    "    sns.lineplot(x=x, y=x*coefficient+intercept, label=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c17ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c1de1ea62f82c461137635ead164383",
     "grade": true,
     "grade_id": "cell-6d4551054dc85e8f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(threshold_table.shape, (5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80911c5b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4aa2d748c80f18e98c44e18920972036",
     "grade": false,
     "grade_id": "cell-0acb2df680f6245a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have experimented a bit with the loss function for linear regression we are going to do some experiments on the (hopefully) familiar `titanic` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6adb29e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc34beb5e8f489f8f8d4c33e35c9a7c2",
     "grade": false,
     "grade_id": "cell-be5d083be1ad3209",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"linreg_intro\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9958ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fff8bc00018e394ada2f307910237f18",
     "grade": false,
     "grade_id": "tita",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Titanic\n",
    "\n",
    "\n",
    "## P1 standardization\n",
    "\n",
    "* Load the titanic dataset.\n",
    "* Impute the age variable, motivate your choice.\n",
    "* Split the dataset into a train and test set, use a 70/30 split.\n",
    "* Now use lineair regression to predict survived from the other numeric variables (visible in the `t.describe()` output). Output the regression weights, and compute Rsquare and RMSE. Just use all numeric columns here (there are 5) although the pclass variable is really a categorical variable.\n",
    "\n",
    "For this question it is OK to use the `LinearRegressionModel` from scikit-learn and the `train_test_split` function as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7761727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic=sns.load_dataset('titanic')\n",
    "print(titanic.shape)\n",
    "print(titanic.head())\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a86c9a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c8fa829f5ad2293b8fffe404368adb1",
     "grade": false,
     "grade_id": "cell-4c66e4f526dd7042",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "titanic['age'] = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(titanic[['age']])\n",
    "\n",
    "# Split the data into test and train datasets, select variables which we do not need to preprocess \n",
    "# Use linear regression to predict survived \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = titanic[['pclass', 'age', 'sibsp', 'parch', 'fare']]\n",
    "y = titanic['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "lin_reg_model = LinearRegression().fit(X_train, y_train)\n",
    "predictions = lin_reg_model.predict(X_test)\n",
    "print(\"['pclass', 'age', 'sibsp', 'parch', 'fare']\", lin_reg_model.coef_)\n",
    "\n",
    "# Compute R-squared and RMSE\n",
    "r2 = r2_score(y_test, predictions)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "#WRITE YOUR CODE HERE\n",
    "print(f\"R-squared: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c7308",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa3c94f50e1a3f2b2a647395d0d9c1df",
     "grade": true,
     "grade_id": "cell-85a381ec7107804e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert X_train.shape == (623, 5)\n",
    "assert X_test.shape == (268, 5)\n",
    "assert y_train.shape == (623,)\n",
    "assert y_test.shape == (268,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa689ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba52a6c05cb86b8863284351cb0ca2b6",
     "grade": false,
     "grade_id": "cell-3596a710148f4797",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "* Now z-transform all explanatory variables and do the same. For this exercise, fit the scalar on the train data and use it to transform both the train and test sets.\n",
    "* Describe what you observe and what you may learn from this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936caaf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2512ca28a7a82c69b3a6774fad8cb06",
     "grade": false,
     "grade_id": "cell-161430fa64250cca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use the StandardScaler class from scikit-learn to z-transform the explanatory variables:\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_test_scaled = StandardScaler().fit(X_train).transform(X_test)\n",
    "lin_reg_model = LinearRegression().fit(X_train_scaled, y_train)\n",
    "predictions_z = lin_reg_model.predict(X_test_scaled)\n",
    "print(\"['pclass', 'age', 'sibsp', 'parch', 'fare']\", lin_reg_model.coef_)\n",
    "\n",
    "# Get predictions on the survived, then compute R-squared and RMSE\n",
    "r2_scaled = r2_score(y_test, predictions_z)\n",
    "rmse_scaled = mean_squared_error(y_test, predictions_z, squared=False)\n",
    "#WRITE YOUR CODE HERE\n",
    "print(f\"R-squared (scaled): {r2_scaled}\")\n",
    "print(f\"RMSE (scaled): {rmse_scaled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be1deb6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ab01af154c185efdbfaff5889a87342",
     "grade": true,
     "grade_id": "cell-e5a7e5db1f81fa17",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert X_train_scaled.shape == (623, 5)\n",
    "assert X_test_scaled.shape == (268, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c784c90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bcf550f89fed4fad669c0154d0cbfb7",
     "grade": false,
     "grade_id": "cell-1e8b5036b5dea702",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"crossval\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193501ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17a5574f30b64fd38b56e8ec959191c5",
     "grade": false,
     "grade_id": "cell-2c488a0cc1405f61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the next two questions, we are going to be working with our scaled data!\n",
    "\n",
    "## P2 cross validation\n",
    "Instead of dividing our dataset in a train and test set, we are going to do 5 fold cross validation with our scaled data, and report the values that we got for this.\n",
    "\n",
    "* Now just take the whole dataset and implement 5 fold cross validation yourself using sklearn.\n",
    "* Then do a linear regression 5 times on the obtained train-test fold splits.\n",
    "* Plot the found evaluation values, and compute the mean and std.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f81d4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "111c8027c21e608c35afb6bb1049bd97",
     "grade": false,
     "grade_id": "cell-19f6a5af1e8853f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#We will use the KFold function from sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42) # don't change the random state\n",
    "\n",
    "#create lists to store scores for r2 and rsme, split the data as usual, predict survival \n",
    "#as before with linear regression, compute our r2 and rsme and append these to lists, plot in your own way.\n",
    "r2_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_index, test_index in kf.split(titanic):\n",
    "    X_train_split, X_test_split = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_split, y_test_split = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    lin_reg_model = LinearRegression().fit(X_train_split, y_train_split)\n",
    "    predictions_f = lin_reg_model.predict(X_test_split)\n",
    "\n",
    "    r2_scores.append(r2_score(y_test_split, predictions_f))\n",
    "    rmse_scores.append(mean_squared_error(y_test_split, predictions_f, squared=False))\n",
    "\n",
    "# plot the scores\n",
    "plt.plot(r2_scores, label='R-squared')\n",
    "plt.plot(rmse_scores, label='RMSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# mean and std\n",
    "print(f\"R-squared mean: {np.mean(r2_scores)}\")\n",
    "print(f\"R-squared std: {np.std(r2_scores)}\")\n",
    "print(f\"RMSE mean: {np.mean(rmse_scores)}\")\n",
    "print(f\"RMSE std: {np.std(rmse_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90d06d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0c5ce05e8bf87b139d4634fefaca268",
     "grade": true,
     "grade_id": "cell-fe50db8646893fb2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.shape(r2_scores), (5,))\n",
    "assert np.allclose(np.shape(rmse_scores), (5,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10244141",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcbdca0b25020bf7ee82cbda708488cd",
     "grade": false,
     "grade_id": "cell-09f01d80ce275f62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"gridsearch\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce654bc9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a0bf0bcf2ba72fbbd42d082358bc73c",
     "grade": false,
     "grade_id": "cell-6145c0ebc236e4df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## P3 Grid search\n",
    "\n",
    "* Split titanic into train, validation, test using a 60/20/20 split for train, validation and test respectively.\n",
    "* Use the validation set to find optimal values for the regularization parameter, and the choice between lasso and ridge.\n",
    "* You are not allowed to use the `GridSearchCV` function, as this does not really do what we want, so\n",
    "we will be implementing it from scratch ourselves. We have already given you a list with three options for the regularization that you should use.\n",
    "* Use the r2 score to measure how well a particular setting works, and save this in a dictionary `results` with as keys `lasso` and `ridge` and as values the values for the regularization parameter in the order given in the list of possible values.\n",
    "\n",
    "HINT: sklearn does not have a function to directly split a dataset into train, validation and test, but you can use `train_test_split` twice, once to create separate train and test sets, and then once more to split the train set into a train and validation portion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91714781",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4d49c59ad0b46ab63afc199fdc0bf2c",
     "grade": false,
     "grade_id": "cell-b281ddd48a267b60",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use these functions for Lasso and Ridge\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Split data into training, validation, and test sets (remember to scale features!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_val_scaled = StandardScaler().fit_transform(X_val)\n",
    "X_test_scaled = StandardScaler().fit(X_train).transform(X_test)\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "alpha_options = [0.1, 1.0, 10.0]\n",
    "\n",
    "# Perform grid search using train to the normalized train to train a model\n",
    "# reporting the validation r2 in the table, calculated on the validation set. \n",
    "# you can use the `r2_score` from sklearn to calculate the rqaured error\n",
    "results = {'ridge': [], 'lasso': []}\n",
    "\n",
    "for alpha_option in alpha_options:\n",
    "    ridge_model = Ridge(alpha=alpha_option).fit(X_train_scaled, y_train)\n",
    "    ridge_predictions = ridge_model.predict(X_val_scaled)\n",
    "    results['ridge'].append(r2_score(y_val, ridge_predictions))\n",
    "\n",
    "    lasso_model = Lasso(alpha=alpha_option).fit(X_train_scaled, y_train)\n",
    "    lasso_predictions = lasso_model.predict(X_val_scaled)\n",
    "    results['lasso'].append(r2_score(y_val, lasso_predictions))\n",
    "    \n",
    "scores_dataframe = pd.DataFrame(results)\n",
    "scores_dataframe.index = alpha_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40d3d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30564b13179e75ede3cefdf0b109e99c",
     "grade": true,
     "grade_id": "cell-8348d5ad1879143c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert X_train.shape == (534, 5)\n",
    "assert y_train.shape == (534,)\n",
    "assert X_val.shape == (178, 5)\n",
    "assert y_val.shape == (178,)\n",
    "assert X_test.shape == (179, 5)\n",
    "assert y_test.shape == (179,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c10f36",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e7bfff9b19aa73796cbe2f4049a23d0",
     "grade": false,
     "grade_id": "cell-64470a0c30149974",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "* After having done this on the validaion set, run all of these possibilities on the test set. \n",
    "* Think, display, and write a conclusion and what you have learned. \n",
    "* You can reuse most of the code from above to do this, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2da2a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "291b1f89eb2051f3c1618a3992b374ce",
     "grade": true,
     "grade_id": "cell-3378f871f1b158d5",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for grid search\n",
    "alpha_options = [0.1, 1.0, 10.0]\n",
    "\n",
    "# Perform grid search using train to the normalized train to train a model\n",
    "# reporting the test r2 in the table, calculated on the test set. \n",
    "# you can use the `r2_score` from sklearn to calculate the rqaured error\n",
    "test_results = {'ridge': [], 'lasso': []}\n",
    "\n",
    "for alpha_option in alpha_options:\n",
    "    ridge_model = Ridge(alpha=alpha_option).fit(X_train_scaled, y_train)\n",
    "    ridge_predictions = ridge_model.predict(X_test_scaled)\n",
    "    test_results['ridge'].append(r2_score(y_test, ridge_predictions))\n",
    "\n",
    "    lasso_model = Lasso(alpha=alpha_option).fit(X_train_scaled, y_train)\n",
    "    lasso_predictions = lasso_model.predict(X_test_scaled)\n",
    "    test_results['lasso'].append(r2_score(y_test, lasso_predictions))\n",
    "test_scores_dataframe = pd.DataFrame(test_results)\n",
    "test_scores_dataframe.index = alpha_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cdc7b8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6946a39113632d15ad328557b3d254a8",
     "grade": true,
     "grade_id": "cell-814da92eec63891e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Validation scores\")\n",
    "print(scores_dataframe)\n",
    "print(\"Test scores\")\n",
    "print(test_scores_dataframe)\n",
    "\n",
    "# conclusion\n",
    "# ridge is much better than lasso for this dataset\n",
    "# the best alpha is 1.0 because it has the best r2 scores when taking into account both validation and test results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
